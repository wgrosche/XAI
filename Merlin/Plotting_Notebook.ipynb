{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3af944",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "Notebook dedicated to plotting the results of the explainer calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load plotting data\n",
    "\n",
    "data_df = pd.read_csv(\"filepath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b02586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load neural network\n",
    "\n",
    "model = tf.keras.models.load_model(\"Models/ml_model_\"+suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate NN performance\n",
    "\n",
    "\"\"\"\n",
    "Evaluate Model\n",
    "\"\"\"\n",
    "\n",
    "# evaluate the fitting validation and training losses\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "## Make Prdictions on the Test Dataset\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=['xt','vt'])\n",
    "\n",
    "pred_norm = np.linalg.norm(y_pred[['xt','vt']].values,axis=1)\n",
    "true_norm = np.linalg.norm(y_test[['xt','vt']].values,axis=1)\n",
    "hist_data = np.abs(pred_norm-true_norm)/np.abs(true_norm)\n",
    "hist_data = pd.DataFrame(hist_data, columns=['norm'])\n",
    "\n",
    "def Remove_Outlier_Indices(df):\n",
    "    Q1 = df.quantile(0.00)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    trueList = ~((df > (Q3 + 1.5 * IQR)))\n",
    "    #trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "    return trueList\n",
    "\n",
    "indices = Remove_Outlier_Indices(hist_data)\n",
    "hist_data = hist_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d89bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "f, axs = plt.subplots(1, 1, figsize=(4, 4), gridspec_kw=dict(width_ratios=[4]))\n",
    "\n",
    "sns.lineplot(data = history.history, x = epochs, y='loss',ax=axs, label='loss')\n",
    "sns.lineplot(data = history.history, x = epochs, y='val_loss',ax=axs, label='val_loss')\n",
    "\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.set_title(\"Losses by Epoch\")\n",
    "axs.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57984db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Error\n",
    "\n",
    "f, axs = plt.subplots(1, 1, figsize=(4, 4), gridspec_kw=dict(width_ratios=[4]))\n",
    "\n",
    "# Error Plot for ML Predictions\n",
    "sns.histplot(data=hist_data, x = 'norm', kde=False, stat='probability', binwidth=0.01, ax=axs)\n",
    "\n",
    "axs.set(xlim=(0, 0.6), ylim=(0, 0.4))\n",
    "axs.set_xlabel('Error')\n",
    "axs.set_ylabel('Probability')\n",
    "axs.set_title(r\"Error Plot $(\\frac{||y_{pred}-y_{true}||}{||y_{true}||})$\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plot ML Model\n",
    "f, axs = plt.subplots(2, 1, figsize=(8, 4), gridspec_kw=dict(width_ratios=[4, 4]))\n",
    "\n",
    "\n",
    "# True Values Plot\n",
    "sns.scatterplot(data = y_test.iloc[:,:], x = 'xt', y='vt',ax=axs[0],label='true_values', marker='x', linewidth = 1)\n",
    "\n",
    "axs[1,0].set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs[1,0].set_xlabel('x [ ]')\n",
    "axs[1,0].set_ylabel('v [ ]')\n",
    "axs[1,0].set_title(\"Phase Space Plot (True Model)\")\n",
    "\n",
    "# ML Values Plot\n",
    "sns.scatterplot(data = y_pred.iloc[:2000,:], x='xt', y='vt',ax=axs[1],label='pred_values',  marker='x', linewidth = 1)\n",
    "\n",
    "axs[1,1].set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs[1,1].set_xlabel('x [ ]')\n",
    "axs[1,1].set_ylabel('v [ ]')\n",
    "axs[1,1].set_title(\"Phase Space Plot (ML Model)\")\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig(\"Images/model_summary\"+suffix+\".svg\", dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74f6a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual feature attributions\n",
    "\n",
    "# Individual Feature Contributions\n",
    "f, axs = plt.subplots(2, 4, figsize = (16,8), gridspec_kw = dict(width_ratios = [4,4,4,4]))\n",
    "\n",
    "sns.scatterplot(x='x0', y=\"xt_x0\", data=big_df, ax=axs[0,0], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "\n",
    "axs[0,0].set_title(r\"$x_0$\")\n",
    "axs[0,0].set_xlabel('Index [ ]')\n",
    "axs[0,0].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='v0', y=\"xt_v0\", data=big_df, ax=axs[1,0], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "axs[1,0].set_title(r\"$v_0$\")\n",
    "axs[1,0].set_xlabel('Index [ ]')\n",
    "axs[1,0].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='t', y=\"xt_t\", data=big_df, ax=axs[0,1], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "axs[0,1].set_title(r\"$t$\")\n",
    "axs[0,1].set_xlabel('Index [ ]')\n",
    "axs[0,1].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='rand', y=\"xt_rand\", data=big_df, ax=axs[1,1], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "\n",
    "axs[1,1].set_title(r\"$rand$\")\n",
    "axs[1,1].set_xlabel('Index [ ]')\n",
    "axs[1,1].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "\n",
    "sns.scatterplot(x='x0', y=\"vt_x0\", data=big_df, ax=axs[0,2], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "\n",
    "axs[0,2].set_title(r\"$x_0$\")\n",
    "axs[0,2].set_xlabel('Index [ ]')\n",
    "axs[0,2].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='v0', y=\"vt_v0\", data=big_df, ax=axs[1,2], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "axs[1,2].set_title(r\"$v_0$\")\n",
    "axs[1,2].set_xlabel('Index [ ]')\n",
    "axs[1,2].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='v0', y=\"vt_t\", data=big_df, ax=axs[0,3], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "axs[0,3].set_title(r\"$t$\")\n",
    "axs[0,3].set_xlabel('Index [ ]')\n",
    "axs[0,3].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.scatterplot(x='rand', y=\"vt_rand\", data=big_df, ax=axs[1,3], marker = 'x', linewidth = 1, hue = \"explainer\")\n",
    "axs[1,3].set_title(r\"$rand$\")\n",
    "axs[1,3].set_xlabel('Index [ ]')\n",
    "axs[1,3].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "\n",
    "\n",
    "f.suptitle(r\"Individual Feature Contributions to $x_t$ and $v_t$\")\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig(\"Images/feature_contributions_sorted_by_feature_\"+suffix+\".svg\", dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45615e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated feature importance\n",
    "\n",
    "# Aggregated\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize = (16,8), gridspec_kw = dict(width_ratios = [4,4]))\n",
    "\n",
    "\n",
    "sns.barplot(data=agg_df_2, x = 'feature', y = np.abs(agg_df_2['value']), hue = 'explainer', ax = axs[0], ci = 95, capsize=.2)\n",
    "\n",
    "axs[0].set_title(r\"Aggregate Feature Contributions\")\n",
    "axs[0].set_xlabel('Feature [ ]')\n",
    "axs[0].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "sns.boxplot(data=agg_df_2, x = 'feature', y = 'value', ax = axs[1], hue = 'explainer')\n",
    "axs[1].set_title(r\"All Feature Contributions\")\n",
    "axs[1].set_xlabel('Feature')\n",
    "axs[1].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig(\"Images/all_aggregated_\"+suffix+\".svg\", dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical explainer convergence study\n",
    "\n",
    "# choose random point from dataset\n",
    "x_val = np.random.choice(choice, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3373ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLime(shap.other.LimeTabular):\n",
    "    def __init__(self, model, data, mode=\"classification\"):\n",
    "        self.model = model\n",
    "        assert mode in [\"classification\", \"regression\"]\n",
    "        self.mode = mode\n",
    "\n",
    "        if str(type(data)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            data = data.values\n",
    "        self.data = data\n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(data, mode=mode)\n",
    "        self.out_dim = 1#self.model(data[0:1]).shape[1]\n",
    "            \n",
    "    def attributions(self, X, num_samples=500, num_features=None):\n",
    "        try:\n",
    "            num_features = X.shape[1] if num_features is None else num_features\n",
    "        except:\n",
    "            print('exception')\n",
    "            num_features = 1\n",
    "        if str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            X = X.values\n",
    "            \n",
    "        out = [np.zeros(X.shape) for j in range(len(self.model))]\n",
    "        for i in tqdm(range(X.shape[0]), desc=\"Calculating Limeâ€¦\", ascii=False, ncols=75):\n",
    "            exp1 = self.explainer.explain_instance(X[i], self.model[0], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            exp2 = self.explainer.explain_instance(X[i], self.model[1], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            for k, v in exp1.local_exp[1]: \n",
    "                out[0][i,k] = v\n",
    "            for k, v in exp2.local_exp[1]: \n",
    "                out[1][i,k] = v\n",
    "          \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime convergence study\n",
    "# use x_val from above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca779b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self,\n",
    "                 training_data,\n",
    "                 mode=\"classification\",\n",
    "                 training_labels=None,\n",
    "                 feature_names=None,\n",
    "                 categorical_features=None,\n",
    "                 categorical_names=None,\n",
    "                 kernel_width=None,\n",
    "                 kernel=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 discretize_continuous=True,\n",
    "                 discretizer='quartile',\n",
    "                 sample_around_instance=False,\n",
    "                 random_state=None,\n",
    "                 training_data_stats=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c81823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
