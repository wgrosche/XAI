{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ece67b",
   "metadata": {},
   "source": [
    "Notebook for convergence study of the step size in the numerical gradient calculation and the LIME kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose point to look at\n",
    "\n",
    "choice = shap.sample(dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Gradient\n",
    "\n",
    "step_range = np.linspace(0,1, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime kernel size\n",
    "\n",
    "kernel_range = np.linspace(0,1,100)\n",
    "\n",
    "\n",
    "for i, kernel_size enumerate(kernel_range):\n",
    "    explainer = MyLime()\n",
    "    feature_atts = explainer.attributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b35d30f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data…: 100%|████████████████████| 10/10 [00:00<00:00, 31.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6344\n",
      "Trained model, loss:  0.63%\n",
      "INFO:tensorflow:Assets written to: Models/ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.5590\n",
      "Trained model, loss:  0.56%\n",
      "INFO:tensorflow:Assets written to: Models/simple_ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "kerneltrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a72abefb82f43c18f6ce71c4c77a99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingtrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb451275895b4c3182217feab9d98973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limetrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Lime…:   0%|                             | 0/2 [00:00<?, ?it/s]Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Calculating Lime…:  50%|██████████▌          | 1/2 [00:00<00:00,  8.26it/s]Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=3.55219e-17): result may not be accurate.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Calculating Lime…: 100%|█████████████████████| 2/2 [00:00<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerictrue\n",
      "kernelml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bcca4f221147c7b9aa5ab0cc849830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e945dc0d99491ba484b76b902f8e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limeml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Lime…:   0%|                             | 0/2 [00:00<?, ?it/s]Ill-conditioned matrix (rcond=3.55219e-17): result may not be accurate.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Calculating Lime…: 100%|█████████████████████| 2/2 [00:00<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numericml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernelsimple\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4158dbfbb09343178922d495f6f88ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingsimple\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e4c963b1b548229acd950cfc9c4b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limesimple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Lime…:   0%|                             | 0/2 [00:00<?, ?it/s]Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.01344e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=3.55219e-17): result may not be accurate.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "Ill-conditioned matrix (rcond=6.4727e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Ill-conditioned matrix (rcond=5.75351e-17): result may not be accurate.\n",
      "Calculating Lime…: 100%|█████████████████████| 2/2 [00:00<00:00, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numericsimple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Data…: 100%|████████████████████| 10/10 [00:00<00:00, 57.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 667us/step - loss: 31.2399\n",
      "Trained model, loss: 31.24%\n",
      "INFO:tensorflow:Assets written to: Models/ml_model_random_feature_params_1.0_-0.5_0.37_0.3_1.2\\assets\n",
      "4/4 [==============================] - 0s 667us/step - loss: 29.0123\n",
      "Trained model, loss: 29.01%\n",
      "INFO:tensorflow:Assets written to: Models/simple_ml_model_random_feature_params_1.0_-0.5_0.37_0.3_1.2\\assets\n",
      "kerneltrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2ea719ae674806ba09892a98fa434b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingtrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4577b6d25d504d6aa281a0bfbd896fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d2cfca57aaa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sampling\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSamplingExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mtemp_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lime\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyLime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlime_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'regression'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_kernel.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_instance_with_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mexplanations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;31m# vector-output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_sampling.py\u001b[0m in \u001b[0;36mexplain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaryingInds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                     \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_estimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                     \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnsamples_each1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_sampling.py\u001b[0m in \u001b[0;36msampling_estimate\u001b[1;34m(self, j, f, x, X, nsamples)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mX_masked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mevals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_masked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0mevals_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mevals_off\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-d2cfca57aaa3>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             traj = solve_ivp(self.eom, [0, X_temp['t'].iloc[i]], [X_temp['x0'].iloc[i], X_temp['v0'].iloc[i]], \n\u001b[0m\u001b[0;32m    175\u001b[0m                             t_eval = None, events = [self.termination_event])\n\u001b[0;32m    176\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py\u001b[0m in \u001b[0;36msolve_ivp\u001b[1;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'finished'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\base.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\u001b[0m in \u001b[0;36m_step_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mh_abs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             y_new, f_new = rk_step(self.fun, t, y, self.f, h, self.A,\n\u001b[0m\u001b[0;32m    145\u001b[0m                                    self.B, self.C, self.K)\n\u001b[0;32m    146\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\u001b[0m in \u001b[0;36mrk_step\u001b[1;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from scipy.fft import fft\n",
    "\n",
    "\n",
    "import shap as shap\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular    \n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import keras\n",
    "\n",
    "# for reproducibility of this notebook:\n",
    "rng = np.random.RandomState(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Duffing():\n",
    "    \"\"\"\n",
    "        Class for the Duffing Oscillator\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters = {'alpha': [0.3], 'beta': [-0.1], 'gamma': [0.37], 'delta': [0.3], 'omega': [1.2]}, \n",
    "                 labels = ['xt','vt'], features = ['x0','v0', 't', 'rand'], scaler = None):\n",
    "        \"\"\"\n",
    "            Define Parameter Configuration to Model\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            alpha : float, linear stiffness\n",
    "            beta  : float, non linearity in the restoring force\n",
    "            gamma : float, amplitude of the periodic driving force\n",
    "            delta : float, amount of damping\n",
    "            omega : float, angular frequency of the periodic driving force\n",
    "        \"\"\"   \n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.scaler = scaler\n",
    "        self.parameters = parameters\n",
    "        self.suffix = \"random_feature_params_\"+str(parameters['alpha'])+\"_\"+str(parameters['beta'])+\"_\"+str(parameters['gamma'])+\"_\"+str(parameters['delta'])+\"_\"+str(parameters['omega'])\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "    def eom(self, t, u):\n",
    "        \"\"\"\n",
    "            Duffing Oscillator Equation of Motion\n",
    "\n",
    "            ddx + delta * dx + alpha * x + beta * x**3 = gamma * cos(omega * t)\n",
    "\n",
    "            Input\n",
    "            ----------\n",
    "            u : vector of length 2, (x,v)\n",
    "                Position and Velocity at time t\n",
    "            t : float, the time t\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            [dx,ddx] : Tuple, Time derivatives of \n",
    "                        position and velocity at time t\n",
    "        \"\"\"\n",
    "        x, dx = u[0], u[1]\n",
    "        ddx = (self.parameters['gamma'] * np.cos(self.parameters['omega'] * t) - (self.parameters['delta'] * dx + self.parameters['alpha'] * x + self.parameters['beta'] * x**3))\n",
    "\n",
    "        return [dx,ddx]\n",
    "    \n",
    "    def energy(self, x, v):\n",
    "        return 0.5*v**2 + 0.5*self.parameters['alpha']*x**2 +0.25*self.parameters['beta']*x**4\n",
    "\n",
    "    def termination_event(self, t, y):\n",
    "        \"\"\"\n",
    "            Stops Numerical Integration once points wander too far away\n",
    "        \"\"\"\n",
    "        return (np.abs(y[0]) - 10)*(np.abs(y[1]) - 10)\n",
    "    termination_event.terminal = True\n",
    "\n",
    "\n",
    "    def generate(self, num_samples = int(5e1), samples=10, end_time=100, gridded=False, num_gammas = 1):\n",
    "        \"\"\"\n",
    "            Generates training samples using scipy.integrate.odeint\n",
    "            to calculate the temporal evolution of a Duffing system.\n",
    "    \n",
    "            Samples randomly from x0 in [-2,2], v0 in [-1,1].\n",
    "    \n",
    "            For each set of initial conditions we generate a trajectory.\n",
    "            The trajectory is randomly sampled to generate training\n",
    "            pairs: X = (x0,v0,t), y = (xt,vt)\n",
    "    \n",
    "            Input\n",
    "            ----------\n",
    "            num_samples : int, number of training\n",
    "                            samples to be generated\n",
    "    \n",
    "            Returns\n",
    "            ----------\n",
    "            X : array((num_samples,3)), each entry in the array\n",
    "                is a training sample (x0,v0,t)\n",
    "            y : array((num_samples,2)), each entry in the array\n",
    "                is a target sample (xt,vt)\n",
    "        \"\"\"\n",
    "        self.scaler = None\n",
    "        #Define bounds of the sampling\n",
    "        x_min = -2\n",
    "        x_max = 2\n",
    "        v_min = -2\n",
    "        v_max = 2\n",
    "        #Initialise the output arrays        \n",
    "        X = np.empty((num_samples*samples, len(np.hstack((self.features, self.labels)))))\n",
    "        #Define the t_range to draw from\n",
    "        t_range = np.linspace(0, end_time, 100, endpoint=False)\n",
    "        t_vals = np.sort(np.random.choice(t_range, size = samples, replace=False))\n",
    "\n",
    "        #Generate num_samples samples\n",
    "        for i in tqdm(range(num_samples), desc=\"Generating Data…\", ascii=False, ncols=75):\n",
    "            #Generate random starting positions\n",
    "            x0 = (x_max - x_min) * np.random.random_sample() + x_min\n",
    "            v0 = (v_max - v_min) * np.random.random_sample() + v_min\n",
    "\n",
    "            #Generate a trajectory\n",
    "            trajectory = solve_ivp(self.eom, [0, end_time], [x0,v0], t_eval = t_vals, events = [self.termination_event])\n",
    "            traj_cutoff =  samples - len(trajectory.y[0])\n",
    "            traj_x = np.append(trajectory.y[0].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            traj_v = np.append(trajectory.y[1].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            val_range_low = i*samples\n",
    "            val_range_high = (i+1)*samples\n",
    "            X[val_range_low:val_range_high,:] = np.hstack((x0*np.ones(samples).reshape(-1,1), \n",
    "                                           v0*np.ones(samples).reshape(-1,1),\n",
    "                                           t_vals.reshape(-1,1),\n",
    "                                           np.random.uniform(-1,1,samples).reshape(-1,1),\n",
    "                                           traj_x.reshape(-1,1), \n",
    "                                           traj_v.reshape(-1,1)))\n",
    "        \n",
    "        self.X_df = pd.DataFrame(X, columns = np.hstack((self.features, self.labels)))\n",
    "        return self.X_df\n",
    "\n",
    "    def scale_features(self):\n",
    "        if self.scaler == None:\n",
    "            self.scaler = MinMaxScaler(feature_range=[0,1])\n",
    "            self.X_df[self.features] = self.scaler.fit_transform(self.X_df[self.features].values)\n",
    "        else: return\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.scaler == None:\n",
    "            self.scale_features()\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values), columns=X.columns)\n",
    "        elif type(X) == pd.core.series.Series:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values.reshape(1,-1)), columns=X.index)\n",
    "        elif type(X) == np.ndarray:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X), columns=self.features)\n",
    "\n",
    "        y = np.ones((np.shape(X_temp)[0], 2))\n",
    "        for i in range(0,np.shape(X_temp)[0]):\n",
    "            traj = solve_ivp(self.eom, [0, X_temp['t'].iloc[i]], [X_temp['x0'].iloc[i], X_temp['v0'].iloc[i]], \n",
    "                            t_eval = None, events = [self.termination_event])\n",
    "            y[i] = [traj.y[0][-1], traj.y[1][-1]]\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def predict_x(self, X):\n",
    "        return self.predict(X)[0]\n",
    "\n",
    "    def predict_v(self, X):          \n",
    "        return self.predict(X)[1]\n",
    "\n",
    "        \n",
    "    def vals_to_df(self, values, data, explainer = \"lime\", suffix = None):\n",
    "        df = pd.DataFrame(values[0], columns = [self.labels[0] + \"_\" + i for i in self.features])\n",
    "        df = df.join(pd.DataFrame(values[1], columns = [self.labels[1] + \"_\" + i for i in self.features]))\n",
    "        df = df.join(pd.DataFrame(data.values, columns = self.features))\n",
    "        df.insert(df.shape[1], 'explainer' ,[explainer for _ in range(df.shape[0])])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "class NumericExplainer():\n",
    "    \"\"\"\n",
    "        Pretty Brute force numerical gradient calculation for\n",
    "        explainability of a known function\n",
    "    \"\"\"\n",
    "    def __init__(self, f, features, labels,  h=0.01):\n",
    "        \"\"\"\n",
    "            Initialises with some configurations for the gradient calculation\n",
    "            as well as the function being differentiated.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            f : function that takes a pandas.DataFrame and outputs a 2d np.array.\n",
    "            features : list of features in the pd.DataFrame for which we are to \n",
    "                differentiate f.\n",
    "            labels : list of features in the np.array.\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        \n",
    "    def gradient(self, X_val, feature):\n",
    "        \"\"\"\n",
    "            Numerical Gradient Calculation by way of a CFD method.\n",
    "            Inputs\n",
    "            --------\n",
    "            X_val : pandas.DataFrame with columns: features and values at\n",
    "                which we want to take the numerical gradient.\n",
    "            feature : feature by which we want to differentiate.\n",
    "        \"\"\"\n",
    "        X_prime_plus = X_val.copy()\n",
    "        X_prime_plus.loc[:,(feature)] = X_prime_plus[feature] + self.h\n",
    "        X_prime_minus = X_val.copy()\n",
    "        X_prime_minus.loc[:,(feature)] = X_prime_minus[feature] - self.h\n",
    "        \n",
    "        grad = (self.f(X_prime_plus) - self.f(X_prime_minus))/(2*self.h)\n",
    "        \n",
    "        return grad\n",
    "    def feature_att(self, X):\n",
    "        \"\"\"\n",
    "            Calculates the Gradients for all Entries in X, for each\n",
    "            feature and label combination.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            X : pandas.DataFrame with columns:features and values at\n",
    "                which we want to differentiate.\n",
    "            Returns\n",
    "            --------\n",
    "            self.__atts : [np.array[...],np.array[...]] of gradients at\n",
    "                each of the input points. Calculated for each label and stacked.\n",
    "        \"\"\"\n",
    "        first_run = True\n",
    "        for i,__label in enumerate(self.labels):\n",
    "            grads = self.gradient(X, self.features[0])[:,i]\n",
    "            for __feat in self.features[1:]:\n",
    "                grads = np.vstack((grads,self.gradient(X, __feat)[:,i]))\n",
    "            normalised_grads = np.abs(grads)/np.sum(np.abs(grads),axis=0)\n",
    "            if first_run:\n",
    "                self.__atts = grads.transpose()\n",
    "                self.__normalised = normalised_grads.transpose()\n",
    "                first_run = False\n",
    "            else:\n",
    "                self.__atts = [self.__atts, grads.transpose()]\n",
    "                self.__normalised = [self.__normalised, normalised_grads.transpose()]\n",
    "                        \n",
    "        return self.__atts#, self.__normalised\n",
    "    \n",
    "class Bootstrapper():\n",
    "    def __init__(self, model, data, features, labels, suffix, explainer_type, num_straps = 50, back_size = 100):\n",
    "        self.explainer_type = explainer_type\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.num_straps = num_straps\n",
    "        self.back_size = back_size\n",
    "        self.suffix = suffix\n",
    "        \n",
    "    def bootstrap(self, X):\n",
    "        self.values = np.empty((self.num_straps, len(self.labels), len(self.features)))\n",
    "        self.mean_std_arr = np.empty((2, len(self.labels), len(self.features)))\n",
    "        for i in range(self.num_straps):\n",
    "            background_i = shap.sample(self.data, self.back_size, random_state = np.random.randint(100))\n",
    "            if self.explainer_type == 'kernel':\n",
    "                exp_i = shap.KernelExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'sample':\n",
    "                exp_i = shap.SampleExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'lime':\n",
    "                exp_i = MyLime(self.model, background_i, mode=\"regression\")\n",
    "                shapper = exp_i.attributions(X)\n",
    "            self.values[i,0,:] = shapper[0]\n",
    "            self.values[i,1,:] = shapper[1]\n",
    "        for i in range(len(self.labels)):\n",
    "            for j in range(len(self.features)):\n",
    "                self.mean_std_arr[0, i, j] = np.mean(self.values[:,i,j])\n",
    "                self.mean_std_arr[1, i, j] = np.std(self.values[:,i,j])\n",
    "            \n",
    "        return self.mean_std_arr\n",
    "    \n",
    "    def to_df(self):\n",
    "        self.bootstrap_df = self.x_list.copy()\n",
    "        for k, col in enumerate([\"mean\", \"std\"]):\n",
    "            for j in range(len(self.labels)):\n",
    "                for i in range(len(self.features)):\n",
    "                    self.bootstrap_df.insert(4 + i + j*len(self.features) + k*len(self.features)*len(self.labels), \n",
    "                                             self.features[i] + \"_\" + self.labels[j] + \"_\" + col, \n",
    "                                             self.bootstrap_array[:,k,j,i])\n",
    "        if self.save:\n",
    "            self.bootstrap_df.to_csv(\"Results/\"+self.explainer_type+\"/\"+self.explainer_type+\"bootstrap_vals_\"+self.suffix+\".csv\")\n",
    "        return self.bootstrap_df\n",
    "    def calculate(self, num_samples = 10, save = True):\n",
    "        self.x_list = self.data.iloc[np.sort(np.random.choice(self.data.shape[0], num_samples, replace =False))]\n",
    "        self.bootstrap_array = np.empty((num_samples, 2, len(self.labels), len(self.features)))\n",
    "        for i in tqdm(range(self.x_list.shape[0]), desc=\"Bootstrapping…\", ascii=False, ncols=75):\n",
    "            x_val = self.x_list.iloc[i,:]\n",
    "            self.bootstrap_array[i,:,:,:] = self.bootstrap(x_val)\n",
    "        return self.to_df()\n",
    "    \n",
    "    \n",
    "class MyLime(shap.other.LimeTabular):\n",
    "    def __init__(self, model, data, mode=\"classification\"):\n",
    "        self.model = model\n",
    "        assert mode in [\"classification\", \"regression\"]\n",
    "        self.mode = mode\n",
    "\n",
    "        if str(type(data)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            data = data.values\n",
    "        self.data = data\n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(data, mode=mode)\n",
    "        self.out_dim = 1#self.model(data[0:1]).shape[1]\n",
    "            \n",
    "    def attributions(self, X, num_samples=2, num_features=None):\n",
    "        try:\n",
    "            num_features = X.shape[1] if num_features is None else num_features\n",
    "        except:\n",
    "            print('exception')\n",
    "            num_features = 1\n",
    "        if str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            X = X.values\n",
    "            \n",
    "        out = [np.zeros(X.shape) for j in range(len(self.model))]\n",
    "        for i in tqdm(range(X.shape[0]), desc=\"Calculating Lime…\", ascii=False, ncols=75):\n",
    "            exp1 = self.explainer.explain_instance(X[i], self.model[0], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            exp2 = self.explainer.explain_instance(X[i], self.model[1], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            for k, v in exp1.local_exp[1]: \n",
    "                out[0][i,k] = v\n",
    "            for k, v in exp2.local_exp[1]: \n",
    "                out[1][i,k] = v\n",
    "          \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Define and Create Model\n",
    "\"\"\"\n",
    "def MLModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='sigmoid'),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "  \n",
    "    \n",
    "def SimpleModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "parameter_list = [{'alpha' : 1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.0, 'delta' : 0.3, 'omega' : 0.0},\n",
    "                  {'alpha' : -1.0, 'beta' : -1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 0.0, 'beta' : 0.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.0, 'omega' : 1.2}]\n",
    "\n",
    "\"\"\"\n",
    "Define Parameter Configuration to Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, linear stiffness\n",
    "    beta  : float, non linearity in the restoring force\n",
    "    gamma : float, amplitude of the periodic driving force\n",
    "    delta : float, amount of damping\n",
    "    omega : float, angular frequency of the periodic driving force\n",
    "\"\"\"   \n",
    "for dict_param in parameter_list:\n",
    "    duffing = Duffing(parameters = dict_param)\n",
    "    eom = duffing.eom\n",
    "    suffix = duffing.suffix\n",
    "\n",
    "    end_time = 100\n",
    "    duffing.generate(10, samples = 100, end_time = end_time)\n",
    "    duffing.scale_features()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(duffing.X_df[duffing.features], duffing.X_df[duffing.labels], test_size=0.1, random_state=42)\n",
    "    \n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    \n",
    "    # Create a basic model instance\n",
    "    model = MLModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25),tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)]\n",
    "\n",
    "\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history=model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    model.save(\"Models/ml_model_\"+suffix)\n",
    "    with open('Models/TrainingHistory/'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    def ml_x(X):\n",
    "        return model.predict(X)[:,0]\n",
    "    def ml_v(X):\n",
    "        return model.predict(X)[:,1]\n",
    "    \n",
    "    # Make Simple ML Model\n",
    "    simple_model = SimpleModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history_simple=simple_model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "    with open('Models/TrainingHistory/simple_'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history_simple.history, file_pi)\n",
    "    loss = simple_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    simple_model.save(\"Models/simple_ml_model_\"+suffix)\n",
    "    \n",
    "    def simple_ml_x(X):\n",
    "        return simple_model.predict(X)[:,0]\n",
    "    def simple_ml_v(X):\n",
    "        return simple_model.predict(X)[:,1]\n",
    "\n",
    "    explainers = [\"kernel\", \"sampling\", \"lime\", \"numeric\"]\n",
    "    true_lime = [duffing.predict_x, duffing.predict_v]\n",
    "    ml_lime = [ml_x, ml_v]\n",
    "    simple_lime = [simple_ml_x, simple_ml_v]\n",
    "    models = {\"true\" : duffing.predict, \"ml\" : model.predict, \"simple\" : simple_model.predict}\n",
    "    lime_models = {\"true\" : true_lime, \"ml\" : ml_lime, \"simple\" : simple_lime}\n",
    "\n",
    "    background = shap.sample(X_test, 10)\n",
    "    choice = X.iloc[np.sort(np.random.choice(X_test.shape[0], 2, replace =False))]\n",
    "\n",
    "\n",
    "    big_df = pd.DataFrame()\n",
    "    for model_ in models:\n",
    "        for explainer in explainers:\n",
    "            print(explainer + model_)\n",
    "            if explainer == \"kernel\":\n",
    "                temp_explainer = shap.KernelExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"sampling\":\n",
    "                temp_explainer = shap.SamplingExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"lime\":\n",
    "                temp_explainer = MyLime(lime_models[model_], choice, mode='regression')\n",
    "                temp_vals = temp_explainer.attributions(choice)\n",
    "            elif explainer == \"numeric\":\n",
    "                temp_explainer = NumericExplainer(models[model_], duffing.features, duffing.labels, h = 0.001)\n",
    "                temp_vals = temp_explainer.feature_att(choice)\n",
    "            else:\n",
    "                print(\"not a valid explainer type\")\n",
    "            big_df = big_df.append(duffing.vals_to_df(temp_vals, \n",
    "                                                            choice, explainer = explainer, suffix = suffix))\n",
    "\n",
    "        \n",
    "    big_df.to_csv(\"Results/explainer_dataframe_\"+suffix+\".csv\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfae30a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.iloc[0,:]['x0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44a8b060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48895259, 0.94954974]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duffing.predict(X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48c4c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X) == pd.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02a633c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65311c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x0', 'v0', 't', 'rand'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0,:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa3836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
