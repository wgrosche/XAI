{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfc028b",
   "metadata": {},
   "source": [
    "# Notebook for Exploring Explainable AI in a Physics Setting\n",
    "\n",
    "In this notebook we explore how SHAP explainability deals with a physical system like the Duffing Oscillator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Libraries\n",
    "\"\"\"\n",
    "# General Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# True Model\n",
    "from scipy.integrate import odeint\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Plotting Libraries\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Explainability\n",
    "import lime\n",
    "import shap\n",
    "\n",
    "# Set Seaborn Theme\n",
    "sns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiac-layout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Choose the training data set to be trained on.\n",
    "\n",
    "For general datasets use custom suffix.\n",
    "\"\"\"\n",
    "suffix = \"mac_lots\"#\"delta03_irrelevant\"#\"delayed_traj_delta03\"\n",
    "\n",
    "train = True\n",
    "\n",
    "X_train = pd.read_csv(\"Data/X_train_\"+suffix+\".csv\", header=0, index_col=0)\n",
    "y_train = pd.read_csv(\"Data/y_train_\"+suffix+\".csv\", header=0, index_col=0)\n",
    "X_test = pd.read_csv(\"Data/X_test_\"+suffix+\".csv\", header=0, index_col=0)\n",
    "y_test = pd.read_csv(\"Data/y_test_\"+suffix+\".csv\", header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cf4599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf, axs = plt.subplots(1, 1, figsize=(12, 8), gridspec_kw=dict(width_ratios=[4]))\\n\\nsns.scatterplot(data = y_test.iloc[:10000,:], x = \\'xt\\', y=\\'vt\\', ax=axs, marker=\\'x\\', linewidth = 1)\\n\\naxs.set(xlim=(-2, 2), ylim=(-2, 2))\\naxs.set_xlabel(\\'x [ ]\\')\\naxs.set_ylabel(\\'v [ ]\\')\\naxs.set_title(\"Phase Space Plot\")\\n\\nf.tight_layout()\\n\\nf.savefig(\"Images/data_only_\"+suffix+\".svg\", dpi=\\'figure\\')\\n\\nf, axs = plt.subplots(1, 1, figsize=(12, 8), gridspec_kw=dict(width_ratios=[4]))\\n\\nsns.scatterplot(data = X_test.iloc[:10000,:], x = \\'x0\\', y=\\'v0\\', ax=axs, marker=\\'x\\', linewidth = 1)\\naxs.set(xlim=(-2, 2), ylim=(-2, 2))\\naxs.set_xlabel(\\'x [ ]\\')\\naxs.set_ylabel(\\'v [ ]\\')\\naxs.set_title(\"Phase Space Plot\")\\n\\nf.tight_layout()\\n\\nf.savefig(\"Images/data_only_\"+suffix+\".svg\", dpi=\\'figure\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "f, axs = plt.subplots(1, 1, figsize=(12, 8), gridspec_kw=dict(width_ratios=[4]))\n",
    "\n",
    "sns.scatterplot(data = y_test.iloc[:10000,:], x = 'xt', y='vt', ax=axs, marker='x', linewidth = 1)\n",
    "\n",
    "axs.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs.set_xlabel('x [ ]')\n",
    "axs.set_ylabel('v [ ]')\n",
    "axs.set_title(\"Phase Space Plot\")\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig(\"Images/data_only_\"+suffix+\".svg\", dpi='figure')\n",
    "\n",
    "f, axs = plt.subplots(1, 1, figsize=(12, 8), gridspec_kw=dict(width_ratios=[4]))\n",
    "\n",
    "sns.scatterplot(data = X_test.iloc[:10000,:], x = 'x0', y='v0', ax=axs, marker='x', linewidth = 1)\n",
    "axs.set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs.set_xlabel('x [ ]')\n",
    "axs.set_ylabel('v [ ]')\n",
    "axs.set_title(\"Phase Space Plot\")\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig(\"Images/data_only_\"+suffix+\".svg\", dpi='figure')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd7059",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Optional data preprocessing step, fourier transform.\n",
    "\n",
    "Scale the data to 0 mean and unit variance, this is mainly done to improve the neural network's performance but also ensures that magnitude differences between the features have no impact on explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb58bda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom scipy.fft import fft, fftfreq, fftshift\\n\\n\\nfft(X_test.values)\\n\\nt = np.arange(250)\\n\\nsp = fftshift(fft(X_test.values[:250,:]))\\n\\nfreq = fftshift(fftfreq(t.shape[-1]))\\n\\nplt.plot(freq, sp.real, freq, sp.imag, 'x')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from scipy.fft import fft, fftfreq, fftshift\n",
    "\n",
    "\n",
    "fft(X_test.values)\n",
    "\n",
    "t = np.arange(250)\n",
    "\n",
    "sp = fftshift(fft(X_test.values[:250,:]))\n",
    "\n",
    "freq = fftshift(fftfreq(t.shape[-1]))\n",
    "\n",
    "plt.plot(freq, sp.real, freq, sp.imag, 'x')\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "available-captain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46476691, -0.15646806, -0.52950641],\n",
       "       [-1.08962127,  0.54608022, -0.12767396],\n",
       "       [ 0.42444954,  0.3075383 , -0.18309912],\n",
       "       ...,\n",
       "       [-0.19980822, -1.30346423,  0.37115253],\n",
       "       [ 1.55915821,  1.11694855,  1.56972175],\n",
       "       [ 1.43625866,  1.60450129, -1.52023125]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train.values)\n",
    "scaler.transform(X_train.values, copy=False)\n",
    "scaler.transform(X_test.values, copy = False)\n",
    "\n",
    "#anova_svm = make_pipeline(scaler, model)\n",
    "#anova_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b00aee",
   "metadata": {},
   "source": [
    "## Network Training\n",
    "\n",
    "Here we set up the neural network. We are using a large batch size (5096) to speed up training. Data are shuffled before training because we are using a method to generate the data that causes codependence in adjacent training samples as they belong to the same trajectory. Early stopping criteria are there to stop overfitting though this has rarely been a problem in this setting.\n",
    "\n",
    "The network architecture makes use of a lot of dense (fully connected) layers. This leads to a large number of trainable parameters so care should be taken to ensure that the dataset is large enough to prevent overfitting being a viable strategy.\n",
    "\n",
    "As a validation set we use 20% of the training data.\n",
    "\n",
    "We make use of the Adam optimiser. Using a learning rate of 0.001 seems to work, greater learning rates cause the network to not learn at all and a smaller learning rate slows learning unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c885dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define and Create Model\n",
    "\"\"\"\n",
    "\n",
    "def MLModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='sigmoid'),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "# Create a basic model instance\n",
    "model = MLModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3199a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Weights Path\n",
    "checkpoint_path = \"Networks/training\"+suffix+\"cp1.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb297a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "393/393 [==============================] - 51s 128ms/step - loss: 0.3915 - val_loss: 0.3427\n",
      "\n",
      "Epoch 00001: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 2/500\n",
      "393/393 [==============================] - 52s 132ms/step - loss: 0.2825 - val_loss: 0.2545\n",
      "\n",
      "Epoch 00002: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 3/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.2369 - val_loss: 0.2115\n",
      "\n",
      "Epoch 00003: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 4/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.2135 - val_loss: 0.2061\n",
      "\n",
      "Epoch 00004: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 5/500\n",
      "393/393 [==============================] - 52s 131ms/step - loss: 0.1995 - val_loss: 0.1923\n",
      "\n",
      "Epoch 00005: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 6/500\n",
      "393/393 [==============================] - 51s 131ms/step - loss: 0.1909 - val_loss: 0.1853\n",
      "\n",
      "Epoch 00006: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 7/500\n",
      "393/393 [==============================] - 53s 134ms/step - loss: 0.1852 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00007: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 8/500\n",
      "393/393 [==============================] - 51s 131ms/step - loss: 0.1807 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00008: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 9/500\n",
      "393/393 [==============================] - 51s 131ms/step - loss: 0.1770 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00009: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 10/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1741 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00010: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 11/500\n",
      "393/393 [==============================] - 51s 131ms/step - loss: 0.1714 - val_loss: 0.1652\n",
      "\n",
      "Epoch 00011: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 12/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1690 - val_loss: 0.1691\n",
      "\n",
      "Epoch 00012: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 13/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1668 - val_loss: 0.1616\n",
      "\n",
      "Epoch 00013: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 14/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1649 - val_loss: 0.1684\n",
      "\n",
      "Epoch 00014: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 15/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1631 - val_loss: 0.1679\n",
      "\n",
      "Epoch 00015: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 16/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1612 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00016: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 17/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1601 - val_loss: 0.1563\n",
      "\n",
      "Epoch 00017: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 18/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1586 - val_loss: 0.1509\n",
      "\n",
      "Epoch 00018: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 19/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1572 - val_loss: 0.1538\n",
      "\n",
      "Epoch 00019: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 20/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1559 - val_loss: 0.1557\n",
      "\n",
      "Epoch 00020: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 21/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1545 - val_loss: 0.1532\n",
      "\n",
      "Epoch 00021: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 22/500\n",
      "393/393 [==============================] - 50s 129ms/step - loss: 0.1537 - val_loss: 0.1489\n",
      "\n",
      "Epoch 00022: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 23/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1527 - val_loss: 0.1498\n",
      "\n",
      "Epoch 00023: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 24/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1514 - val_loss: 0.1514\n",
      "\n",
      "Epoch 00024: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 25/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1507 - val_loss: 0.1501\n",
      "\n",
      "Epoch 00025: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 26/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1497 - val_loss: 0.1435\n",
      "\n",
      "Epoch 00026: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 27/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1486 - val_loss: 0.1594\n",
      "\n",
      "Epoch 00027: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 28/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1478 - val_loss: 0.1410\n",
      "\n",
      "Epoch 00028: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 29/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1472 - val_loss: 0.1467\n",
      "\n",
      "Epoch 00029: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 30/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1464 - val_loss: 0.1427\n",
      "\n",
      "Epoch 00030: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 31/500\n",
      "393/393 [==============================] - 49s 126ms/step - loss: 0.1457 - val_loss: 0.1415\n",
      "\n",
      "Epoch 00031: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 32/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1447 - val_loss: 0.1423\n",
      "\n",
      "Epoch 00032: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 33/500\n",
      "393/393 [==============================] - 49s 126ms/step - loss: 0.1440 - val_loss: 0.1411\n",
      "\n",
      "Epoch 00033: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 34/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1431 - val_loss: 0.1449\n",
      "\n",
      "Epoch 00034: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 35/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1427 - val_loss: 0.1446\n",
      "\n",
      "Epoch 00035: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 36/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1423 - val_loss: 0.1397\n",
      "\n",
      "Epoch 00036: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 37/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1415 - val_loss: 0.1374\n",
      "\n",
      "Epoch 00037: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 38/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1409 - val_loss: 0.1412\n",
      "\n",
      "Epoch 00038: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 39/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1402 - val_loss: 0.1368\n",
      "\n",
      "Epoch 00039: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 40/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1400 - val_loss: 0.1364\n",
      "\n",
      "Epoch 00040: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 41/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1393 - val_loss: 0.1387\n",
      "\n",
      "Epoch 00041: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 42/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1388 - val_loss: 0.1461\n",
      "\n",
      "Epoch 00042: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 43/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1380 - val_loss: 0.1460\n",
      "\n",
      "Epoch 00043: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 44/500\n",
      "393/393 [==============================] - 49s 126ms/step - loss: 0.1378 - val_loss: 0.1371\n",
      "\n",
      "Epoch 00044: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 45/500\n",
      "393/393 [==============================] - 49s 126ms/step - loss: 0.1373 - val_loss: 0.1379\n",
      "\n",
      "Epoch 00045: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 46/500\n",
      "393/393 [==============================] - 49s 126ms/step - loss: 0.1367 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00046: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 47/500\n",
      "393/393 [==============================] - 52s 133ms/step - loss: 0.1363 - val_loss: 0.1330\n",
      "\n",
      "Epoch 00047: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 48/500\n",
      "393/393 [==============================] - 51s 131ms/step - loss: 0.1359 - val_loss: 0.1363\n",
      "\n",
      "Epoch 00048: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 49/500\n",
      "393/393 [==============================] - 52s 133ms/step - loss: 0.1354 - val_loss: 0.1360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00049: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 50/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1353 - val_loss: 0.1341\n",
      "\n",
      "Epoch 00050: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 51/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1351 - val_loss: 0.1375\n",
      "\n",
      "Epoch 00051: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 52/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1341 - val_loss: 0.1313\n",
      "\n",
      "Epoch 00052: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 53/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1339 - val_loss: 0.1376\n",
      "\n",
      "Epoch 00053: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 54/500\n",
      "393/393 [==============================] - 50s 126ms/step - loss: 0.1333 - val_loss: 0.1325\n",
      "\n",
      "Epoch 00054: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 55/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1335 - val_loss: 0.1400\n",
      "\n",
      "Epoch 00055: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 56/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1329 - val_loss: 0.1383\n",
      "\n",
      "Epoch 00056: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 57/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1323 - val_loss: 0.1427\n",
      "\n",
      "Epoch 00057: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 58/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1319 - val_loss: 0.1275\n",
      "\n",
      "Epoch 00058: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 59/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1318 - val_loss: 0.1422\n",
      "\n",
      "Epoch 00059: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 60/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1312 - val_loss: 0.1326\n",
      "\n",
      "Epoch 00060: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 61/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1312 - val_loss: 0.1453\n",
      "\n",
      "Epoch 00061: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 62/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1308 - val_loss: 0.1342\n",
      "\n",
      "Epoch 00062: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 63/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1306 - val_loss: 0.1315\n",
      "\n",
      "Epoch 00063: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 64/500\n",
      "393/393 [==============================] - 50s 127ms/step - loss: 0.1301 - val_loss: 0.1327\n",
      "\n",
      "Epoch 00064: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 65/500\n",
      "393/393 [==============================] - 50s 128ms/step - loss: 0.1299 - val_loss: 0.1338\n",
      "\n",
      "Epoch 00065: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 66/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1293 - val_loss: 0.1292\n",
      "\n",
      "Epoch 00066: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 67/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1289 - val_loss: 0.1245\n",
      "\n",
      "Epoch 00067: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 68/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1291 - val_loss: 0.1271\n",
      "\n",
      "Epoch 00068: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 69/500\n",
      "393/393 [==============================] - 52s 131ms/step - loss: 0.1288 - val_loss: 0.1338\n",
      "\n",
      "Epoch 00069: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 70/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1284 - val_loss: 0.1283\n",
      "\n",
      "Epoch 00070: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 71/500\n",
      "393/393 [==============================] - 51s 129ms/step - loss: 0.1278 - val_loss: 0.1276\n",
      "\n",
      "Epoch 00071: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 72/500\n",
      "393/393 [==============================] - 51s 130ms/step - loss: 0.1278 - val_loss: 0.1256\n",
      "\n",
      "Epoch 00072: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 73/500\n",
      "393/393 [==============================] - 53s 134ms/step - loss: 0.1275 - val_loss: 0.1297\n",
      "\n",
      "Epoch 00073: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 74/500\n",
      "393/393 [==============================] - 52s 132ms/step - loss: 0.1273 - val_loss: 0.1270\n",
      "\n",
      "Epoch 00074: saving model to Networks\\trainingmac_lotscp1.ckpt\n",
      "Epoch 75/500\n",
      "357/393 [==========================>...] - ETA: 4s - loss: 0.1271"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train Model\n",
    "\"\"\"\n",
    "#model.build()\n",
    "# Display the model's architecture\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "if train:\n",
    "    # Create a callback that saves the model's weights\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "    callbacks = [cp_callback,\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15),\n",
    "                 tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)]\n",
    "\n",
    "    pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history=model.fit(X_train, y_train, steps_per_epoch=None, epochs=500, \n",
    "                      validation_split=0.2, batch_size=20364, shuffle=True, callbacks=callbacks, verbose=1)\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load Model\n",
    "\"\"\"\n",
    "if not train:\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(\"Untrained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    # Loads the weights\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    # Re-evaluate the model\n",
    "    loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueModel():\n",
    "    \"\"\"\n",
    "    Class to represent the True Model of the Duffing oscillator.\n",
    "    Uses the scipy odeint integrator to perform time evolution.\n",
    "    \n",
    "    Methods:\n",
    "    -----------\n",
    "    predict(X):\n",
    "        Inputs:\n",
    "        --------\n",
    "        X: pandas DataFrame with at least columns x0,v0,t\n",
    "        Returns:\n",
    "        --------\n",
    "        y: pandas DataFrame with columns xt,vt\n",
    "    \"\"\"\n",
    "    def __init__(self, scaler, X):\n",
    "        self.alpha=-1\n",
    "        self.beta=1\n",
    "        self.delta=0.3\n",
    "        self.gamma=0.37\n",
    "        self.omega=1.2\n",
    "        self.scaler = scaler\n",
    "        self.cols = X.columns\n",
    "        \n",
    "    def eom(self, u, t):\n",
    "        x, dx = u[0], u[1]\n",
    "        ddx= self.gamma * np.cos(self.omega * t) - (self.delta * dx + self.alpha*x + self.beta * x**3)\n",
    "        return [dx,ddx]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = pd.DataFrame(self.scaler.inverse_transform(X), columns=self.cols)\n",
    "        y = np.ones((np.shape(X)[0], 2))\n",
    "        for i in range(0,np.shape(X)[0]):\n",
    "            t_range = np.linspace(0, X['t'].iloc[i], 500, endpoint=False)\n",
    "            y[i,:] = odeint(self.eom, [X['x0'].iloc[i],X['v0'].iloc[i]], t_range)[-1]\n",
    "        #y = pd.DataFrame(y, columns=['xt','vt'])    \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315e4bc",
   "metadata": {},
   "source": [
    "## Evaluation and Visualisation of Results:\n",
    "\n",
    "To be able to visualise the error we remove outliers (top 5% of absolute error). This ensures that the rare misclassifications don't drag the x-axis of our error plot out too far.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the fitting validation and training losses\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "## Make Prdictions on the Test Dataset\n",
    "y_pred = pd.DataFrame(model.predict(X_test), columns=['xt','vt'])\n",
    "\n",
    "pred_norm = np.linalg.norm(y_pred[['xt','vt']].values,axis=1)\n",
    "true_norm = np.linalg.norm(y_test[['xt','vt']].values,axis=1)\n",
    "hist_data = np.abs(pred_norm-true_norm)/np.abs(true_norm)\n",
    "hist_data = pd.DataFrame(hist_data, columns=['norm'])\n",
    "\n",
    "def Remove_Outlier_Indices(df):\n",
    "    Q1 = df.quantile(0.00)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    trueList = ~((df > (Q3 + 1.5 * IQR)))\n",
    "    #trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "    return trueList\n",
    "\n",
    "indices = Remove_Outlier_Indices(hist_data)\n",
    "hist_data = hist_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a11f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plot ML Model\n",
    "f, axs = plt.subplots(2, 2, figsize=(12, 8), gridspec_kw=dict(width_ratios=[4, 4]))\n",
    "\n",
    "sns.lineplot(data = history.history, x = epochs, y='loss',ax=axs[0,0], label='loss')\n",
    "sns.lineplot(data = history.history, x = epochs, y='val_loss',ax=axs[0,0], label='val_loss')\n",
    "\n",
    "axs[0,0].set_xlabel('Epochs')\n",
    "axs[0,0].set_ylabel('Loss')\n",
    "axs[0,0].set_title(\"Losses by Epoch\")\n",
    "axs[0,0].legend()\n",
    "\n",
    "# Error Plot for ML Predictions\n",
    "sns.histplot(data=hist_data, x = 'norm', kde=False, stat='probability', binwidth=0.01, ax=axs[0,1])\n",
    "\n",
    "axs[0,1].set(xlim=(0, 0.6), ylim=(0, 0.4))\n",
    "axs[0,1].set_xlabel('Error')\n",
    "axs[0,1].set_ylabel('Probability')\n",
    "axs[0,1].set_title(r\"Error Plot $(\\frac{||y_{pred}-y_{true}||}{||y_{true}||})$\")\n",
    "\n",
    "# True Values Plot\n",
    "sns.scatterplot(data = y_test.iloc[:2000,:], x = 'xt', y='vt',ax=axs[1,0],label='true_values', marker='x', linewidth = 1)\n",
    "\n",
    "axs[1,0].set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs[1,0].set_xlabel('x [ ]')\n",
    "axs[1,0].set_ylabel('v [ ]')\n",
    "axs[1,0].set_title(\"Phase Space Plot (True Model)\")\n",
    "\n",
    "# ML Values Plot\n",
    "sns.scatterplot(data = y_pred.iloc[:2000,:], x='xt', y='vt',ax=axs[1,1],label='pred_values',  marker='x', linewidth = 1)\n",
    "\n",
    "axs[1,1].set(xlim=(-2, 2), ylim=(-2, 2))\n",
    "axs[1,1].set_xlabel('x [ ]')\n",
    "axs[1,1].set_ylabel('v [ ]')\n",
    "axs[1,1].set_title(\"Phase Space Plot (ML Model)\")\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "f.savefig(\"Images/model_summary\"+suffix+\".svg\", dpi='figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7141be",
   "metadata": {},
   "source": [
    "### Ordered \n",
    "\n",
    "Lets choose a spectrum of values that have similar v0 and t whilst x0 varies from -2 to 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136be1f",
   "metadata": {},
   "source": [
    "## Good Values\n",
    "Only looking at good predictions: $|y_{pred}-y_{true}| < 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab202aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explainers: Here we create the explainer object and evaluate it on a number of samples\n",
    "tol: Tolerance for y_pred-y_true\n",
    "num_vals: Number of values to evaluate the explainers on. Use minimum of num_vals and number of samples available\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class wilke_explainer():\n",
    "    \"\"\"\n",
    "        Class to evaluate models and plot both the individual and aggregate \n",
    "        feature importance. Implements a method for shap and for lime.\n",
    "        Implements methods from the SHAP library, tailored for this notebook.\n",
    "    \"\"\"\n",
    "    def __init__(self, models, background_data, test_data, test_labels, data_tol=0.1, num_vals=100, \n",
    "                 explainer_type='shap', background_resolution=100, tolerance=0.1):\n",
    "        \"\"\"\n",
    "            Initialises the explainer objects for the various models.\n",
    "            \n",
    "            Parameters\n",
    "            ----------\n",
    "            models : dictionary of models that implement a predict function\n",
    "                that take a 2d numpy.array or pandas.DataFrame\n",
    "                \n",
    "            background_data : pandas.DataFrame with samples of training data\n",
    "        \"\"\"\n",
    "        self.num_vals = num_vals\n",
    "        self.tol = data_tol\n",
    "        self.models = models\n",
    "        self.background_data = background_data\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.tolerance = tolerance\n",
    "        self.explainers = {}\n",
    "        self.background = shap.sample(self.background_data, background_resolution)\n",
    "        self.explainer_type = explainer_type\n",
    "        if explainer_type=='shap':\n",
    "             for mod in models:\n",
    "                self.explainers[mod] = shap.KernelExplainer((models[mod]).predict, self.background)\n",
    "        if explainer_type=='lime':\n",
    "             for mod in models:\n",
    "                self.explainers[mod] = shap.LimeTabular((models[mod]).predict, background_resolution, mode=\"regression\")\n",
    "    \n",
    "    def choose_data(self, i, feature, num_features):\n",
    "        vals = np.abs(np.linalg.norm((self.models['ml']).predict(self.test_data), axis=1) - \n",
    "                      np.linalg.norm(self.test_labels, axis=1))\n",
    "        data_arr = self.test_data.iloc[np.where(vals < self.tolerance)]\n",
    "        where__ = np.ones_like(data_arr.values[:,i], dtype=bool)\n",
    "        for j in range(1,num_features):\n",
    "            where__ = np.multiply(where__, np.abs(data_arr.values[:,(i + j)%num_features])<self.tol)\n",
    "        data_arr = data_arr.iloc[where__]\n",
    "        data_arr = data_arr.iloc[np.sort(\n",
    "            np.random.choice(data_arr.shape[0], np.min([self.num_vals, data_arr.shape[0]]), replace=False))]\n",
    "        return data_arr\n",
    "        \n",
    "    def eval_explainer(self):\n",
    "        first_run = True\n",
    "        for i, __feature in enumerate(self.test_data.columns):\n",
    "            arr = self.choose_data(i, __feature, len(self.test_data.columns))\n",
    "            for __explainer in self.explainers:\n",
    "                if self.explainer_type=='shap':\n",
    "                    __atts = self.explainers[__explainer].shap_values(arr)\n",
    "                if self.explainer_type=='lime':\n",
    "                    __atts = self.explainers[__explainer].attributions(arr)\n",
    "                    \n",
    "                for j, __contribution in enumerate(self.test_labels.columns):\n",
    "                    multi_index = [range(len(arr)), [__feature for i in range(len(arr))], \n",
    "                                   [__contribution for i in range(len(arr))],\n",
    "                                   [__explainer for i in range(len(arr))]]\n",
    "                    if first_run:\n",
    "                        self.feature_attributions = pd.DataFrame(__atts[j], \n",
    "                                                                 columns = self.test_data.columns, \n",
    "                                                                 index = pd.MultiIndex.from_arrays(multi_index, \n",
    "                                                                        names=('num', 'feature', 'contribution', 'model')))\n",
    "                    else:\n",
    "                        self.feature_attributions = self.feature_attributions.append(pd.DataFrame(__atts[j], \n",
    "                                                                 columns = self.test_data.columns, \n",
    "                                                                 index = pd.MultiIndex.from_arrays(multi_index, \n",
    "                                                                        names=('num', 'feature', 'contribution', 'model'))))\n",
    "                    first_run = False\n",
    "                    \n",
    "        return self.feature_attributions\n",
    "        \n",
    "    def exp_plot(self):\n",
    "        f, axs = plt.subplots(self.test_labels.shape[1], self.test_data.shape[1], \n",
    "                              figsize=(4*self.test_data.shape[1], 8), \n",
    "                              gridspec_kw=dict(width_ratios=4*np.ones((self.test_data.shape[1]))))\n",
    "\n",
    "        for i, __feature in enumerate(self.test_data.columns):\n",
    "            for j, __contribution in enumerate(self.test_labels.columns):\n",
    "                for __model in self.models:\n",
    "                    sns.scatterplot(data = self.feature_attributions.xs((__feature, __contribution, __model), \n",
    "                                                      level=('feature', 'contribution', 'model')), \n",
    "                                    x = self.feature_attributions.xs((__feature, __contribution, 'true'), \n",
    "                                                   level=('feature', 'contribution', 'model')).index,\n",
    "                                    y=self.feature_attributions.xs((__feature, __contribution, __model), \n",
    "                                                 level=('feature', 'contribution', 'model'))[__feature],\n",
    "                                    label = __model, ax=axs[j,i])  \n",
    "                    \n",
    "                axs[j,i].set_title(r\"Feature Contribution of \"+feature+\" to \"+contribution+\"\")\n",
    "                axs[j,i].set_xlabel('Index [ ]')\n",
    "                axs[j,i].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "        f.tight_layout()\n",
    "\n",
    "        f.savefig(\"Images/shap_summary\"+suffix+\"_kernel_good.svg\", dpi='figure')\n",
    "    \n",
    "    def agg_func(self, X):\n",
    "        return np.mean(np.abs(X))\n",
    "    \n",
    "    def aggregate(self):       \n",
    "        self.agg_vals = np.zeros((len(self.test_labels.columns),len(self.models)))\n",
    "        \n",
    "        for i, __contribution in enumerate(self.test_labels.columns):\n",
    "            for j, __model in enumerate(self.models):\n",
    "                for k, __feature in enumerate(self.test_data.columns):\n",
    "                    self.agg_vals[i,j] = self.agg_func(\n",
    "                        self.feature_attributions.xs((__feature,\n",
    "                                                      __contribution,\n",
    "                                                      __model), level=('feature', 'contribution', 'model'))[__feature].values)\n",
    "\n",
    "        self.agg_vals = pd.DataFrame(self.agg_vals, columns = self.test_labels.columns, index = list(self.models.keys()))\n",
    "        return self.agg_vals\n",
    "    \n",
    "    def agg_plot(self):\n",
    "        f, axs = plt.subplots(len(self.models), self.test_labels.shape[1], \n",
    "                              figsize=(4*self.test_labels.shape[1], 4*len(self.models)), \n",
    "                              gridspec_kw=dict(width_ratios=4*np.ones((self.test_labels.shape[1]))))\n",
    "        \n",
    "        for i, __model in enumerate(self.models):\n",
    "            for j, __contribution in enumerate(self.test_labels):\n",
    "                sns.barplot(data = self.agg_vals, x = self.agg_vals.index,\n",
    "                            y = __contribution, label = contribution, ax=axs[i,j])\n",
    "                axs[i,j].set_title(r\"Aggregate Feature Contribution to \"+__contribution+\" in the \"+__model+\" Model\")\n",
    "                axs[i,j].set_ylabel('Feature Contribution [ ]')\n",
    "\n",
    "        f.tight_layout()\n",
    "        f.savefig(\"Images/shap_aggregated\"+suffix+\".svg\", dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_model = TrueModel(scaler, X_test)\n",
    "\n",
    "models = {'ml': model, \n",
    "         'true': true_model}\n",
    "\n",
    "explainer = wilke_explainer(models, X_train, X_test, y_test, num_vals = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_data = explainer.eval_explainer()\n",
    "\n",
    "explainer.exp_plot()\n",
    "\n",
    "explainer_agg = explainer.aggregate()\n",
    "\n",
    "explainer.agg_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43204723",
   "metadata": {},
   "source": [
    "## Lime\n",
    "\n",
    "Using SHAP's LimeTabular for explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = wilke_explainer(models, X_train, X_test, y_test, explainer_type='lime')\n",
    "\n",
    "explainer_data = explainer.eval_explainer()\n",
    "\n",
    "explainer.exp_plot()\n",
    "\n",
    "explainer_agg = explainer.aggregate()\n",
    "\n",
    "explainer.agg_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233de79f",
   "metadata": {},
   "source": [
    "## An Analytic Approach to Explaining Feature Importance\n",
    "\n",
    "In order to evaluate the usefulness of SHAP in explaining feature importance we need to determine what we would expect the explainer to give us.\n",
    "\n",
    "The initial approach to doing this will be to take the gradient in each feature direction (x0,v0,t) at each point.\n",
    "\n",
    "Since we cannot calculate the gradient analytically this will be done numerically.\n",
    "\n",
    "To do so we take a point and calculate how much it shifts based on changing the input parameters:\n",
    "\n",
    "$f(x_0+\\delta,v_0,t) = (x,v),\\quad f(x_0-\\delta,v_0,t) = (x',v')  ,\\quad \\frac{df}{dx_0} = \\frac{|(x',v')-(x,v)|}{\\delta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_grad(f, X, delta=0.01):\n",
    "    \"\"\"\n",
    "        Numerical Gradient Calculation by Perturbing the \n",
    "        Inputs using a Central Finite Difference Method\n",
    "\n",
    "        df/dx0 = |f(x0+delta,v0,t)-f(x0-delta,v0,t)|/(2*delta)\n",
    "\n",
    "        Input\n",
    "        ----------\n",
    "        f : function to differentiate\n",
    "        u : vector of length 2, (x,v)\n",
    "            Position and Velocity at time t\n",
    "        t : float, the time t\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : float, perturbation\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        [df/dx0,df/dv0] : Tuple, Derivatives of f by \n",
    "                    starting position and velocity\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    x01df = pd.DataFrame(np.array([X['x0']+delta, X['v0'],X['t']]), columns=['x0','v0','t'])\n",
    "    x02df = pd.DataFrame(np.array([X['x0']-delta, X['v0'],X['t']]), columns=['x0','v0','t'])\n",
    "    v01df = pd.DataFrame(np.array([X['x0'], X['v0']+delta,X['t']]), columns=['x0','v0','t'])\n",
    "    v02df = pd.DataFrame(np.array([X['x0'], X['v0']-delta,X['t']]), columns=['x0','v0','t'])\n",
    "    \n",
    "    dx0 = (f(x01df) - f(x02df))/(2*delta)\n",
    "    \n",
    "    dv0 = (f(v01df) - f(v02df))/(2*delta)\n",
    "    \n",
    "    return pd.DataFrame(np.array([dx0, dv0]), columns=['dx0', 'dv0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e06168",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grad(true_model.predict, X, delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a8f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d2d9dea",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Plan: \n",
    "    Finish gradient calc\n",
    "    Plots:\n",
    "            xt against x0, t, v0. individually. same for vt\n",
    "            feature importance with x0 ordered etc\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
