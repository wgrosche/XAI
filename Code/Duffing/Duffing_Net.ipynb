{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prompt-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contemporary-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen= DuffingGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, reuse = None):\n",
    "    hidden1 = tf.layers.dense(inputs=z, units=32, activation = tf.nn.leaky_relu)\n",
    "    hidden2 = tf.layers.dense(inputs=hidden1, units=32, activation=tf.nn.leaky_relu)\n",
    "    output=tf.layers.dense(inputs=hidden2, units=64, activation=tf.nn.tanh)\n",
    "    return output\n",
    "\n",
    "def discriminator(X, reuse= None):\n",
    "    with tf.variable_scope('dis', reuse=reuse):\n",
    "        hidden1 = tf.layers.dense(inputs=X, units=32, activation = tf.nn.leaky_relu)\n",
    "        hidden2 = tf.layers.dense(inputs=hidden1, units=32, activation=tf.nn.leaky_relu)\n",
    "        logits = tf.layers.dense(hidden2, units =1)\n",
    "        output = tf.sigmoid(logits)\n",
    "        return output, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating placeholders for the outputs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "real_images = tf.placeholder(tf.float32, shape =[None, 784])\n",
    "z = tf.placeholder(tf.float32, shape =[None, 100])\n",
    "\n",
    "G = generator(z)\n",
    "D_output_real, D_logits_real = discriminator(real_images)\n",
    "D_output_fake, D_logits_fake = discriminator(G, reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the loss function\n",
    "def loss_func(logits_in, labels_in):\n",
    "\treturn tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "\t\t\t\t\t\tlogits = logits_in, labels = labels_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries and the MNIST dataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Smoothing for generalization\n",
    "D_real_loss = loss_func(D_logits_real, tf.ones_like(D_logits_real)*0.9)\n",
    "D_fake_loss = loss_func(D_logits_fake, tf.zeros_like(D_logits_real))\n",
    "D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "G_loss = loss_func(D_logits_fake, tf.ones_like(D_logits_fake))\n",
    "\n",
    "# defining the learning rate, batch size,\n",
    "# number of epochs and using the Adam optimizer\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "# Do this when multiple networks\n",
    "# interact with each other\n",
    "\n",
    "# returns all variables created(the two\n",
    "# variable scopes) and makes trainable true\n",
    "tvars = tf.trainable_variables()\n",
    "d_vars =[var for var in tvars if 'dis' in var.name]\n",
    "g_vars =[var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "D_trainer = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list = d_vars)\n",
    "G_trainer = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list = g_vars)\n",
    "\n",
    "batch_size = 100 # batch size\n",
    "epochs = 500 # number of epochs. The higher the better the result\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# creating a session to train the networks\n",
    "samples =[] # generator examples\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(init)\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tnum_batches = mnist.train.num_examples//batch_size\n",
    "\t\t\n",
    "\t\tfor i in range(num_batches):\n",
    "\t\t\tbatch = mnist.train.next_batch(batch_size)\n",
    "\t\t\tbatch_images = batch[0].reshape((batch_size, 784))\n",
    "\t\t\tbatch_images = batch_images * 2-1\n",
    "\t\t\tbatch_z = np.random.uniform(-1, 1, size =(batch_size, 100))\n",
    "\t\t\t_= sess.run(D_trainer, feed_dict ={real_images:batch_images, z:batch_z})\n",
    "\t\t\t_= sess.run(G_trainer, feed_dict ={z:batch_z})\n",
    "\t\t\t\n",
    "\t\tprint(\"on epoch{}\".format(epoch))\n",
    "\t\t\n",
    "\t\tsample_z = np.random.uniform(-1, 1, size =(1, 100))\n",
    "\t\tgen_sample = sess.run(generator(z, reuse = True),\n",
    "\t\t\t\t\t\t\t\tfeed_dict ={z:sample_z})\n",
    "\t\t\n",
    "\t\tsamples.append(gen_sample)\n",
    "\n",
    "# result after 0th epoch\n",
    "plt.imshow(samples[0].reshape(28, 28))\n",
    "\n",
    "# result after 499th epoch\n",
    "plt.imshow(samples[49].reshape(28, 28))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
