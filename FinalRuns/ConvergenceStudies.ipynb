{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ece67b",
   "metadata": {},
   "source": [
    "Notebook for convergence study of the step size in the numerical gradient calculation and the LIME kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose point to look at\n",
    "\n",
    "choice = shap.sample(dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Gradient\n",
    "\n",
    "step_range = np.linspace(0,1, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime kernel size\n",
    "\n",
    "kernel_range = np.linspace(0,1,100)\n",
    "\n",
    "\n",
    "for i, kernel_size enumerate(kernel_range):\n",
    "    explainer = MyLime()\n",
    "    feature_atts = explainer.attributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35d30f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data…: 100%|████████████████████| 10/10 [00:00<00:00, 28.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6344\n",
      "Trained model, loss:  0.63%\n",
      "INFO:tensorflow:Assets written to: Models/ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5590\n",
      "Trained model, loss:  0.56%\n",
      "INFO:tensorflow:Assets written to: Models/simple_ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "kerneltrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a5dcb76f4d45c9b2779f65a5e4fe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samplingtrue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8684bb353dd245a9924f12064fb8c6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d2cfca57aaa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sampling\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSamplingExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mtemp_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lime\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyLime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlime_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'regression'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_kernel.py\u001b[0m in \u001b[0;36mshap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_instance_with_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                 \u001b[0mexplanations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;31m# vector-output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_sampling.py\u001b[0m in \u001b[0;36mexplain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaryingInds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                     \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_estimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                     \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnsamples_each1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnsamples_each2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\shap\\explainers\\_sampling.py\u001b[0m in \u001b[0;36msampling_estimate\u001b[1;34m(self, j, f, x, X, nsamples)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mX_masked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mevals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_masked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[0mevals_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mevals_off\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d2cfca57aaa3>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             traj = solve_ivp(self.eom, [0, X_temp['t'].iloc[i]], [X_temp['x0'].iloc[i], X_temp['v0'].iloc[i]], \n\u001b[0m\u001b[0;32m    175\u001b[0m                             t_eval = None, events = [self.termination_event])\n\u001b[0;32m    176\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtraj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py\u001b[0m in \u001b[0;36msolve_ivp\u001b[1;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'finished'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\base.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\u001b[0m in \u001b[0;36m_step_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mh_abs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             y_new, f_new = rk_step(self.fun, t, y, self.f, h, self.A,\n\u001b[0m\u001b[0;32m    145\u001b[0m                                    self.B, self.C, self.K)\n\u001b[0;32m    146\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\u001b[0m in \u001b[0;36mrk_step\u001b[1;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mdy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from scipy.fft import fft\n",
    "\n",
    "\n",
    "import shap as shap\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular    \n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import keras\n",
    "\n",
    "# for reproducibility of this notebook:\n",
    "rng = np.random.RandomState(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Duffing():\n",
    "    \"\"\"\n",
    "        Class for the Duffing Oscillator\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters = {'alpha': [0.3], 'beta': [-0.1], 'gamma': [0.37], 'delta': [0.3], 'omega': [1.2]}, \n",
    "                 labels = ['xt','vt'], features = ['x0','v0', 't', 'rand'], scaler = None):\n",
    "        \"\"\"\n",
    "            Define Parameter Configuration to Model\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            alpha : float, linear stiffness\n",
    "            beta  : float, non linearity in the restoring force\n",
    "            gamma : float, amplitude of the periodic driving force\n",
    "            delta : float, amount of damping\n",
    "            omega : float, angular frequency of the periodic driving force\n",
    "        \"\"\"   \n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.scaler = scaler\n",
    "        self.parameters = parameters\n",
    "        self.suffix = \"random_feature_params_\"+str(parameters['alpha'])+\"_\"+str(parameters['beta'])+\"_\"+str(parameters['gamma'])+\"_\"+str(parameters['delta'])+\"_\"+str(parameters['omega'])\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "    def eom(self, t, u):\n",
    "        \"\"\"\n",
    "            Duffing Oscillator Equation of Motion\n",
    "\n",
    "            ddx + delta * dx + alpha * x + beta * x**3 = gamma * cos(omega * t)\n",
    "\n",
    "            Input\n",
    "            ----------\n",
    "            u : vector of length 2, (x,v)\n",
    "                Position and Velocity at time t\n",
    "            t : float, the time t\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            [dx,ddx] : Tuple, Time derivatives of \n",
    "                        position and velocity at time t\n",
    "        \"\"\"\n",
    "        x, dx = u[0], u[1]\n",
    "        ddx = (self.parameters['gamma'] * np.cos(self.parameters['omega'] * t) - (self.parameters['delta'] * dx + self.parameters['alpha'] * x + self.parameters['beta'] * x**3))\n",
    "\n",
    "        return [dx,ddx]\n",
    "    \n",
    "    def energy(self, x, v):\n",
    "        return 0.5*v**2 + 0.5*self.parameters['alpha']*x**2 +0.25*self.parameters['beta']*x**4\n",
    "\n",
    "    def termination_event(self, t, y):\n",
    "        \"\"\"\n",
    "            Stops Numerical Integration once points wander too far away\n",
    "        \"\"\"\n",
    "        return (np.abs(y[0]) - 10)*(np.abs(y[1]) - 10)\n",
    "    termination_event.terminal = True\n",
    "\n",
    "\n",
    "    def generate(self, num_samples = int(5e1), samples=10, end_time=100, gridded=False, num_gammas = 1):\n",
    "        \"\"\"\n",
    "            Generates training samples using scipy.integrate.odeint\n",
    "            to calculate the temporal evolution of a Duffing system.\n",
    "    \n",
    "            Samples randomly from x0 in [-2,2], v0 in [-1,1].\n",
    "    \n",
    "            For each set of initial conditions we generate a trajectory.\n",
    "            The trajectory is randomly sampled to generate training\n",
    "            pairs: X = (x0,v0,t), y = (xt,vt)\n",
    "    \n",
    "            Input\n",
    "            ----------\n",
    "            num_samples : int, number of training\n",
    "                            samples to be generated\n",
    "    \n",
    "            Returns\n",
    "            ----------\n",
    "            X : array((num_samples,3)), each entry in the array\n",
    "                is a training sample (x0,v0,t)\n",
    "            y : array((num_samples,2)), each entry in the array\n",
    "                is a target sample (xt,vt)\n",
    "        \"\"\"\n",
    "        self.scaler = None\n",
    "        #Define bounds of the sampling\n",
    "        x_min = -2\n",
    "        x_max = 2\n",
    "        v_min = -2\n",
    "        v_max = 2\n",
    "        #Initialise the output arrays        \n",
    "        X = np.empty((num_samples*samples, len(np.hstack((self.features, self.labels)))))\n",
    "        #Define the t_range to draw from\n",
    "        t_range = np.linspace(0, end_time, 100, endpoint=False)\n",
    "        t_vals = np.sort(np.random.choice(t_range, size = samples, replace=False))\n",
    "\n",
    "        #Generate num_samples samples\n",
    "        for i in tqdm(range(num_samples), desc=\"Generating Data…\", ascii=False, ncols=75):\n",
    "            #Generate random starting positions\n",
    "            x0 = (x_max - x_min) * np.random.random_sample() + x_min\n",
    "            v0 = (v_max - v_min) * np.random.random_sample() + v_min\n",
    "\n",
    "            #Generate a trajectory\n",
    "            trajectory = solve_ivp(self.eom, [0, end_time], [x0,v0], t_eval = t_vals, events = [self.termination_event])\n",
    "            traj_cutoff =  samples - len(trajectory.y[0])\n",
    "            traj_x = np.append(trajectory.y[0].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            traj_v = np.append(trajectory.y[1].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            val_range_low = i*samples\n",
    "            val_range_high = (i+1)*samples\n",
    "            X[val_range_low:val_range_high,:] = np.hstack((x0*np.ones(samples).reshape(-1,1), \n",
    "                                           v0*np.ones(samples).reshape(-1,1),\n",
    "                                           t_vals.reshape(-1,1),\n",
    "                                           np.random.uniform(-1,1,samples).reshape(-1,1),\n",
    "                                           traj_x.reshape(-1,1), \n",
    "                                           traj_v.reshape(-1,1)))\n",
    "        \n",
    "        self.X_df = pd.DataFrame(X, columns = np.hstack((self.features, self.labels)))\n",
    "        return self.X_df\n",
    "\n",
    "    def scale_features(self):\n",
    "        if self.scaler == None:\n",
    "            self.scaler = MinMaxScaler(feature_range=[0,1])\n",
    "            self.X_df[self.features] = self.scaler.fit_transform(self.X_df[self.features].values)\n",
    "        else: return\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.scaler == None:\n",
    "            self.scale_features()\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values), columns=X.columns)\n",
    "        elif type(X) == pd.core.series.Series:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values.reshape(1,-1)), columns=X.index)\n",
    "        elif type(X) == np.ndarray:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X), columns=self.features)\n",
    "\n",
    "        y = np.ones((np.shape(X_temp)[0], 2))\n",
    "        for i in range(0,np.shape(X_temp)[0]):\n",
    "            traj = solve_ivp(self.eom, [0, X_temp['t'].iloc[i]], [X_temp['x0'].iloc[i], X_temp['v0'].iloc[i]], \n",
    "                            t_eval = None, events = [self.termination_event])\n",
    "            y[i] = [traj.y[0][-1], traj.y[1][-1]]\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def predict_x(self, X):\n",
    "        return self.predict(X)[0]\n",
    "\n",
    "    def predict_v(self, X):          \n",
    "        return self.predict(X)[1]\n",
    "\n",
    "        \n",
    "    def vals_to_df(self, values, data, explainer = \"lime\", suffix = None):\n",
    "        df = pd.DataFrame(values[0], columns = [self.labels[0] + \"_\" + i for i in self.features])\n",
    "        df = df.join(pd.DataFrame(values[1], columns = [self.labels[1] + \"_\" + i for i in self.features]))\n",
    "        df = df.join(pd.DataFrame(data.values, columns = self.features))\n",
    "        df.insert(df.shape[1], 'explainer' ,[explainer for _ in range(df.shape[0])])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "class NumericExplainer():\n",
    "    \"\"\"\n",
    "        Pretty Brute force numerical gradient calculation for\n",
    "        explainability of a known function\n",
    "    \"\"\"\n",
    "    def __init__(self, f, features, labels,  h=0.01):\n",
    "        \"\"\"\n",
    "            Initialises with some configurations for the gradient calculation\n",
    "            as well as the function being differentiated.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            f : function that takes a pandas.DataFrame and outputs a 2d np.array.\n",
    "            features : list of features in the pd.DataFrame for which we are to \n",
    "                differentiate f.\n",
    "            labels : list of features in the np.array.\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        \n",
    "    def gradient(self, X_val, feature):\n",
    "        \"\"\"\n",
    "            Numerical Gradient Calculation by way of a CFD method.\n",
    "            Inputs\n",
    "            --------\n",
    "            X_val : pandas.DataFrame with columns: features and values at\n",
    "                which we want to take the numerical gradient.\n",
    "            feature : feature by which we want to differentiate.\n",
    "        \"\"\"\n",
    "        X_prime_plus = X_val.copy()\n",
    "        X_prime_plus.loc[:,(feature)] = X_prime_plus[feature] + self.h\n",
    "        X_prime_minus = X_val.copy()\n",
    "        X_prime_minus.loc[:,(feature)] = X_prime_minus[feature] - self.h\n",
    "        \n",
    "        grad = (self.f(X_prime_plus) - self.f(X_prime_minus))/(2*self.h)\n",
    "        \n",
    "        return grad\n",
    "    def feature_att(self, X):\n",
    "        \"\"\"\n",
    "            Calculates the Gradients for all Entries in X, for each\n",
    "            feature and label combination.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            X : pandas.DataFrame with columns:features and values at\n",
    "                which we want to differentiate.\n",
    "            Returns\n",
    "            --------\n",
    "            self.__atts : [np.array[...],np.array[...]] of gradients at\n",
    "                each of the input points. Calculated for each label and stacked.\n",
    "        \"\"\"\n",
    "        first_run = True\n",
    "        for i,__label in enumerate(self.labels):\n",
    "            grads = self.gradient(X, self.features[0])[:,i]\n",
    "            for __feat in self.features[1:]:\n",
    "                grads = np.vstack((grads,self.gradient(X, __feat)[:,i]))\n",
    "            normalised_grads = np.abs(grads)/np.sum(np.abs(grads),axis=0)\n",
    "            if first_run:\n",
    "                self.__atts = grads.transpose()\n",
    "                self.__normalised = normalised_grads.transpose()\n",
    "                first_run = False\n",
    "            else:\n",
    "                self.__atts = [self.__atts, grads.transpose()]\n",
    "                self.__normalised = [self.__normalised, normalised_grads.transpose()]\n",
    "                        \n",
    "        return self.__atts#, self.__normalised\n",
    "    \n",
    "class Bootstrapper():\n",
    "    def __init__(self, model, data, features, labels, suffix, explainer_type, num_straps = 50, back_size = 100):\n",
    "        self.explainer_type = explainer_type\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.num_straps = num_straps\n",
    "        self.back_size = back_size\n",
    "        self.suffix = suffix\n",
    "        \n",
    "    def bootstrap(self, X):\n",
    "        self.values = np.empty((self.num_straps, len(self.labels), len(self.features)))\n",
    "        self.mean_std_arr = np.empty((2, len(self.labels), len(self.features)))\n",
    "        for i in range(self.num_straps):\n",
    "            background_i = shap.sample(self.data, self.back_size, random_state = np.random.randint(100))\n",
    "            if self.explainer_type == 'kernel':\n",
    "                exp_i = shap.KernelExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'sample':\n",
    "                exp_i = shap.SampleExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'lime':\n",
    "                exp_i = MyLime(self.model, background_i, mode=\"regression\")\n",
    "                shapper = exp_i.attributions(X)\n",
    "            self.values[i,0,:] = shapper[0]\n",
    "            self.values[i,1,:] = shapper[1]\n",
    "        for i in range(len(self.labels)):\n",
    "            for j in range(len(self.features)):\n",
    "                self.mean_std_arr[0, i, j] = np.mean(self.values[:,i,j])\n",
    "                self.mean_std_arr[1, i, j] = np.std(self.values[:,i,j])\n",
    "            \n",
    "        return self.mean_std_arr\n",
    "    \n",
    "    def to_df(self):\n",
    "        self.bootstrap_df = self.x_list.copy()\n",
    "        for k, col in enumerate([\"mean\", \"std\"]):\n",
    "            for j in range(len(self.labels)):\n",
    "                for i in range(len(self.features)):\n",
    "                    self.bootstrap_df.insert(4 + i + j*len(self.features) + k*len(self.features)*len(self.labels), \n",
    "                                             self.features[i] + \"_\" + self.labels[j] + \"_\" + col, \n",
    "                                             self.bootstrap_array[:,k,j,i])\n",
    "        if self.save:\n",
    "            self.bootstrap_df.to_csv(\"Results/\"+self.explainer_type+\"/\"+self.explainer_type+\"bootstrap_vals_\"+self.suffix+\".csv\")\n",
    "        return self.bootstrap_df\n",
    "    def calculate(self, num_samples = 10, save = True):\n",
    "        self.x_list = self.data.iloc[np.sort(np.random.choice(self.data.shape[0], num_samples, replace =False))]\n",
    "        self.bootstrap_array = np.empty((num_samples, 2, len(self.labels), len(self.features)))\n",
    "        for i in tqdm(range(self.x_list.shape[0]), desc=\"Bootstrapping…\", ascii=False, ncols=75):\n",
    "            x_val = self.x_list.iloc[i,:]\n",
    "            self.bootstrap_array[i,:,:,:] = self.bootstrap(x_val)\n",
    "        return self.to_df()\n",
    "    \n",
    "    \n",
    "class MyLime(shap.other.LimeTabular):\n",
    "    def __init__(self, model, data, mode=\"classification\"):\n",
    "        self.model = model\n",
    "        assert mode in [\"classification\", \"regression\"]\n",
    "        self.mode = mode\n",
    "\n",
    "        if str(type(data)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            data = data.values\n",
    "        self.data = data\n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(data, mode=mode)\n",
    "        self.out_dim = 1#self.model(data[0:1]).shape[1]\n",
    "            \n",
    "    def attributions(self, X, num_samples=2, num_features=None):\n",
    "        try:\n",
    "            num_features = X.shape[1] if num_features is None else num_features\n",
    "        except:\n",
    "            print('exception')\n",
    "            num_features = 1\n",
    "        if str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            X = X.values\n",
    "            \n",
    "        out = [np.zeros(X.shape) for j in range(len(self.model))]\n",
    "        for i in tqdm(range(X.shape[0]), desc=\"Calculating Lime…\", ascii=False, ncols=75):\n",
    "            exp1 = self.explainer.explain_instance(X[i], self.model[0], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            exp2 = self.explainer.explain_instance(X[i], self.model[1], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            for k, v in exp1.local_exp[1]: \n",
    "                out[0][i,k] = v\n",
    "            for k, v in exp2.local_exp[1]: \n",
    "                out[1][i,k] = v\n",
    "          \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Define and Create Model\n",
    "\"\"\"\n",
    "def MLModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='sigmoid'),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "  \n",
    "    \n",
    "def SimpleModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "parameter_list = [{'alpha' : 1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.0, 'delta' : 0.3, 'omega' : 0.0},\n",
    "                  {'alpha' : -1.0, 'beta' : -1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 0.0, 'beta' : 0.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.0, 'omega' : 1.2}]\n",
    "\n",
    "\"\"\"\n",
    "Define Parameter Configuration to Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, linear stiffness\n",
    "    beta  : float, non linearity in the restoring force\n",
    "    gamma : float, amplitude of the periodic driving force\n",
    "    delta : float, amount of damping\n",
    "    omega : float, angular frequency of the periodic driving force\n",
    "\"\"\"   \n",
    "for dict_param in parameter_list:\n",
    "    duffing = Duffing(parameters = dict_param)\n",
    "    eom = duffing.eom\n",
    "    suffix = duffing.suffix\n",
    "\n",
    "    end_time = 100\n",
    "    duffing.generate(10, samples = 100, end_time = end_time)\n",
    "    duffing.scale_features()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(duffing.X_df[duffing.features], duffing.X_df[duffing.labels], test_size=0.1, random_state=42)\n",
    "    \n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    \n",
    "    # Create a basic model instance\n",
    "    model = MLModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25),tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)]\n",
    "\n",
    "\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history=model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    model.save(\"Models/ml_model_\"+suffix)\n",
    "    with open('Models/TrainingHistory/'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    def ml_x(X):\n",
    "        return model.predict(X)[:,0]\n",
    "    def ml_v(X):\n",
    "        return model.predict(X)[:,1]\n",
    "    \n",
    "    # Make Simple ML Model\n",
    "    simple_model = SimpleModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history_simple=simple_model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "    with open('Models/TrainingHistory/simple_'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history_simple.history, file_pi)\n",
    "    loss = simple_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    simple_model.save(\"Models/simple_ml_model_\"+suffix)\n",
    "    \n",
    "    def simple_ml_x(X):\n",
    "        return simple_model.predict(X)[:,0]\n",
    "    def simple_ml_v(X):\n",
    "        return simple_model.predict(X)[:,1]\n",
    "\n",
    "    explainers = [\"kernel\", \"sampling\", \"lime\", \"numeric\"]\n",
    "    true_lime = [duffing.predict_x, duffing.predict_v]\n",
    "    ml_lime = [ml_x, ml_v]\n",
    "    simple_lime = [simple_ml_x, simple_ml_v]\n",
    "    models = {\"true\" : duffing.predict, \"ml\" : model.predict, \"simple\" : simple_model.predict}\n",
    "    lime_models = {\"true\" : true_lime, \"ml\" : ml_lime, \"simple\" : simple_lime}\n",
    "\n",
    "    background = shap.sample(X_test, 10)\n",
    "    choice = X.iloc[np.sort(np.random.choice(X_test.shape[0], 2, replace =False))]\n",
    "\n",
    "\n",
    "    big_df = pd.DataFrame()\n",
    "    for model_ in models:\n",
    "        for explainer in explainers:\n",
    "            print(explainer + model_)\n",
    "            if explainer == \"kernel\":\n",
    "                temp_explainer = shap.KernelExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"sampling\":\n",
    "                temp_explainer = shap.SamplingExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"lime\":\n",
    "                temp_explainer = MyLime(lime_models[model_], choice, mode='regression')\n",
    "                temp_vals = temp_explainer.attributions(choice)\n",
    "            elif explainer == \"numeric\":\n",
    "                temp_explainer = NumericExplainer(models[model_], duffing.features, duffing.labels, h = 0.001)\n",
    "                temp_vals = temp_explainer.feature_att(choice)\n",
    "            else:\n",
    "                print(\"not a valid explainer type\")\n",
    "            big_df = big_df.append(duffing.vals_to_df(temp_vals, \n",
    "                                                            choice, explainer = explainer, suffix = suffix))\n",
    "\n",
    "        \n",
    "    big_df.to_csv(\"Results/explainer_dataframe_\"+suffix+\".csv\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfae30a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.iloc[0,:]['x0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a8b060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'duffing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ddd3941d7132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mduffing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'duffing' is not defined"
     ]
    }
   ],
   "source": [
    "duffing.predict(X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48c4c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X) == pd.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02a633c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65311c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['x0', 'v0', 't', 'rand'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0,:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafa3836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data…: 100%|██████████████████| 100/100 [00:02<00:00, 35.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5617\n",
      "Trained model, loss:  0.56%\n",
      "INFO:tensorflow:Assets written to: Models/ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5247\n",
      "Trained model, loss:  0.52%\n",
      "INFO:tensorflow:Assets written to: Models/simple_ml_model_random_feature_params_1.0_1.0_0.37_0.3_1.2\\assets\n",
      "limetrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Lime…:   0%|                            | 0/10 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [500, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f342d190d73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lime\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyLime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlime_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'regression'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                 \u001b[0mtemp_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_explainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"numeric\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m                 \u001b[0mtemp_explainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNumericExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduffing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduffing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9f342d190d73>\u001b[0m in \u001b[0;36mattributions\u001b[1;34m(self, X, num_samples, num_features)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Calculating Lime…\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             exp1 = self.explainer.explain_instance(X[i], self.model[0], labels=range(self.out_dim), \n\u001b[0m\u001b[0;32m    342\u001b[0m                                                     num_features=num_features, num_samples=num_samples)\n\u001b[0;32m    343\u001b[0m             exp2 = self.explainer.explain_instance(X[i], self.model[1], labels=range(self.out_dim), \n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\lime\\lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    450\u001b[0m             (ret_exp.intercept[label],\n\u001b[0;32m    451\u001b[0m              \u001b[0mret_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_exp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m              \u001b[0mret_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance_with_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m                     \u001b[0mscaled_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m                     \u001b[0myss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\lime\\lime_base.py\u001b[0m in \u001b[0;36mexplain_instance_with_data\u001b[1;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mlabels_column\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighborhood_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         used_features = self.feature_selection(neighborhood_data,\n\u001b[0m\u001b[0;32m    184\u001b[0m                                                \u001b[0mlabels_column\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                                                \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\lime\\lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[1;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[0mn_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'highest_weights'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             return self.feature_selection(data, labels, weights,\n\u001b[0m\u001b[0;32m    135\u001b[0m                                           num_features, n_method)\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\lime\\lime_base.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[1;34m(self, data, labels, weights, num_features, method)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'forward_selection'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'highest_weights'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             clf = Ridge(alpha=0.01, fit_intercept=True,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\lime\\lime_base.py\u001b[0m in \u001b[0;36mforward_selection\u001b[1;34m(self, data, labels, weights, num_features)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mused_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 clf.fit(data[:, used_features + [feature]], labels,\n\u001b[0m\u001b[0;32m     60\u001b[0m                         sample_weight=weights)\n\u001b[0;32m     61\u001b[0m                 score = clf.score(data[:, used_features + [feature]],\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \"\"\"\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    540\u001b[0m         _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X),\n\u001b[0;32m    541\u001b[0m                                                   self.solver)\n\u001b[1;32m--> 542\u001b[1;33m         X, y = self._validate_data(X, y,\n\u001b[0m\u001b[0;32m    543\u001b[0m                                    \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_accept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\XAI\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [500, 2]"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from scipy.integrate import odeint, solve_ivp\n",
    "from scipy.fft import fft\n",
    "\n",
    "\n",
    "import shap as shap\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_tabular    \n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import keras\n",
    "\n",
    "# for reproducibility of this notebook:\n",
    "rng = np.random.RandomState(42)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Duffing():\n",
    "    \"\"\"\n",
    "        Class for the Duffing Oscillator\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters = {'alpha': [0.3], 'beta': [-0.1], 'gamma': [0.37], 'delta': [0.3], 'omega': [1.2]}, \n",
    "                 labels = ['xt','vt'], features = ['x0','v0', 't', 'energy'], scaler = None):\n",
    "        \"\"\"\n",
    "            Define Parameter Configuration to Model\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            alpha : float, linear stiffness\n",
    "            beta  : float, non linearity in the restoring force\n",
    "            gamma : float, amplitude of the periodic driving force\n",
    "            delta : float, amount of damping\n",
    "            omega : float, angular frequency of the periodic driving force\n",
    "        \"\"\"   \n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        self.scaler = scaler\n",
    "        self.parameters = parameters\n",
    "        self.suffix = \"random_feature_params_\"+str(parameters['alpha'])+\"_\"+str(parameters['beta'])+\"_\"+str(parameters['gamma'])+\"_\"+str(parameters['delta'])+\"_\"+str(parameters['omega'])\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "    def eom(self, t, u):\n",
    "        \"\"\"\n",
    "            Duffing Oscillator Equation of Motion\n",
    "\n",
    "            ddx + delta * dx + alpha * x + beta * x**3 = gamma * cos(omega * t)\n",
    "\n",
    "            Input\n",
    "            ----------\n",
    "            u : vector of length 2, (x,v)\n",
    "                Position and Velocity at time t\n",
    "            t : float, the time t\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            [dx,ddx] : Tuple, Time derivatives of \n",
    "                        position and velocity at time t\n",
    "        \"\"\"\n",
    "        x, dx = u[0], u[1]\n",
    "        ddx = (self.parameters['gamma'] * np.cos(self.parameters['omega'] * t) - (self.parameters['delta'] * dx + self.parameters['alpha'] * x + self.parameters['beta'] * x**3))\n",
    "\n",
    "        return [dx,ddx]\n",
    "    \n",
    "    def energy(self, x, v):\n",
    "        return 0.5*v**2 + 0.5*self.parameters['alpha']*x**2 +0.25*self.parameters['beta']*x**4\n",
    "\n",
    "    def termination_event(self, t, y):\n",
    "        \"\"\"\n",
    "            Stops Numerical Integration once points wander too far away\n",
    "        \"\"\"\n",
    "        return (np.abs(y[0]) - 10)*(np.abs(y[1]) - 10)\n",
    "    termination_event.terminal = True\n",
    "\n",
    "\n",
    "    def generate(self, num_samples = int(5e1), samples=10, end_time=100, gridded=False, num_gammas = 1):\n",
    "        \"\"\"\n",
    "            Generates training samples using scipy.integrate.odeint\n",
    "            to calculate the temporal evolution of a Duffing system.\n",
    "    \n",
    "            Samples randomly from x0 in [-2,2], v0 in [-1,1].\n",
    "    \n",
    "            For each set of initial conditions we generate a trajectory.\n",
    "            The trajectory is randomly sampled to generate training\n",
    "            pairs: X = (x0,v0,t), y = (xt,vt)\n",
    "    \n",
    "            Input\n",
    "            ----------\n",
    "            num_samples : int, number of training\n",
    "                            samples to be generated\n",
    "    \n",
    "            Returns\n",
    "            ----------\n",
    "            X : array((num_samples,3)), each entry in the array\n",
    "                is a training sample (x0,v0,t)\n",
    "            y : array((num_samples,2)), each entry in the array\n",
    "                is a target sample (xt,vt)\n",
    "        \"\"\"\n",
    "        self.scaler = None\n",
    "        #Define bounds of the sampling\n",
    "        x_min = -2\n",
    "        x_max = 2\n",
    "        v_min = -2\n",
    "        v_max = 2\n",
    "        #Initialise the output arrays        \n",
    "        X = np.empty((num_samples*samples, len(np.hstack((self.features, self.labels)))))\n",
    "        #Define the t_range to draw from\n",
    "        t_range = np.linspace(0, end_time, 100, endpoint=False)\n",
    "        t_vals = np.sort(np.random.choice(t_range, size = samples, replace=False))\n",
    "\n",
    "        #Generate num_samples samples\n",
    "        for i in tqdm(range(num_samples), desc=\"Generating Data…\", ascii=False, ncols=75):\n",
    "            #Generate random starting positions\n",
    "            x0 = (x_max - x_min) * np.random.random_sample() + x_min\n",
    "            v0 = (v_max - v_min) * np.random.random_sample() + v_min\n",
    "\n",
    "            #Generate a trajectory\n",
    "            trajectory = solve_ivp(self.eom, [0, end_time], [x0,v0], t_eval = t_vals, events = [self.termination_event])\n",
    "            traj_cutoff =  samples - len(trajectory.y[0])\n",
    "            traj_x = np.append(trajectory.y[0].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            traj_v = np.append(trajectory.y[1].reshape(-1,1), 10.0*np.ones(traj_cutoff).reshape(-1,1))\n",
    "            val_range_low = i*samples\n",
    "            val_range_high = (i+1)*samples\n",
    "            X[val_range_low:val_range_high,:] = np.hstack((x0*np.ones(samples).reshape(-1,1), \n",
    "                                           v0*np.ones(samples).reshape(-1,1),\n",
    "                                           t_vals.reshape(-1,1),\n",
    "                                           self.energy(x0, v0)*np.ones(samples).reshape(-1,1),\n",
    "                                           traj_x.reshape(-1,1), \n",
    "                                           traj_v.reshape(-1,1)))\n",
    "        \n",
    "        self.X_df = pd.DataFrame(X, columns = np.hstack((self.features, self.labels)))\n",
    "        return self.X_df\n",
    "\n",
    "    def scale_features(self):\n",
    "        if self.scaler == None:\n",
    "            self.scaler = MinMaxScaler(feature_range=[0,1])\n",
    "            self.X_df[self.features] = self.scaler.fit_transform(self.X_df[self.features].values)\n",
    "        else: return\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.scaler == None:\n",
    "            self.scale_features()\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values), columns=X.columns)\n",
    "        elif type(X) == pd.core.series.Series:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X.values.reshape(1,-1)), columns=X.index)\n",
    "        elif type(X) == np.ndarray:\n",
    "            X_temp = pd.DataFrame(self.scaler.inverse_transform(X), columns=self.features)\n",
    "\n",
    "        y = np.ones((np.shape(X_temp)[0], 2))\n",
    "        for i in range(0,np.shape(X_temp)[0]):\n",
    "            traj = solve_ivp(self.eom, [0, X_temp['t'].iloc[i]], [X_temp['x0'].iloc[i], X_temp['v0'].iloc[i]], \n",
    "                            t_eval = None, events = [self.termination_event])\n",
    "            y[i] = [traj.y[0][-1], traj.y[1][-1]]\n",
    "            \n",
    "        return y\n",
    "\n",
    "    def predict_x(self, X):\n",
    "        return self.predict(X)[0]\n",
    "\n",
    "    def predict_v(self, X):          \n",
    "        return self.predict(X)[1]\n",
    "\n",
    "        \n",
    "    def vals_to_df(self, values, data, explainer = \"lime\", suffix = None):\n",
    "        df = pd.DataFrame(values[0], columns = [self.labels[0] + \"_\" + i for i in self.features])\n",
    "        df = df.join(pd.DataFrame(values[1], columns = [self.labels[1] + \"_\" + i for i in self.features]))\n",
    "        df = df.join(pd.DataFrame(data.values, columns = self.features))\n",
    "        df.insert(df.shape[1], 'explainer' ,[explainer for _ in range(df.shape[0])])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "class NumericExplainer():\n",
    "    \"\"\"\n",
    "        Pretty Brute force numerical gradient calculation for\n",
    "        explainability of a known function\n",
    "    \"\"\"\n",
    "    def __init__(self, f, features, labels,  h=0.01):\n",
    "        \"\"\"\n",
    "            Initialises with some configurations for the gradient calculation\n",
    "            as well as the function being differentiated.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            f : function that takes a pandas.DataFrame and outputs a 2d np.array.\n",
    "            features : list of features in the pd.DataFrame for which we are to \n",
    "                differentiate f.\n",
    "            labels : list of features in the np.array.\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        \n",
    "    def gradient(self, X_val, feature):\n",
    "        \"\"\"\n",
    "            Numerical Gradient Calculation by way of a CFD method.\n",
    "            Inputs\n",
    "            --------\n",
    "            X_val : pandas.DataFrame with columns: features and values at\n",
    "                which we want to take the numerical gradient.\n",
    "            feature : feature by which we want to differentiate.\n",
    "        \"\"\"\n",
    "        X_prime_plus = X_val.copy()\n",
    "        X_prime_plus.loc[:,(feature)] = X_prime_plus[feature] + self.h\n",
    "        X_prime_minus = X_val.copy()\n",
    "        X_prime_minus.loc[:,(feature)] = X_prime_minus[feature] - self.h\n",
    "        \n",
    "        grad = (self.f(X_prime_plus) - self.f(X_prime_minus))/(2*self.h)\n",
    "        \n",
    "        return grad\n",
    "    def feature_att(self, X):\n",
    "        \"\"\"\n",
    "            Calculates the Gradients for all Entries in X, for each\n",
    "            feature and label combination.\n",
    "            \n",
    "            Inputs\n",
    "            --------\n",
    "            X : pandas.DataFrame with columns:features and values at\n",
    "                which we want to differentiate.\n",
    "            Returns\n",
    "            --------\n",
    "            self.__atts : [np.array[...],np.array[...]] of gradients at\n",
    "                each of the input points. Calculated for each label and stacked.\n",
    "        \"\"\"\n",
    "        first_run = True\n",
    "        for i,__label in enumerate(self.labels):\n",
    "            grads = self.gradient(X, self.features[0])[:,i]\n",
    "            for __feat in self.features[1:]:\n",
    "                grads = np.vstack((grads,self.gradient(X, __feat)[:,i]))\n",
    "            normalised_grads = np.abs(grads)/np.sum(np.abs(grads),axis=0)\n",
    "            if first_run:\n",
    "                self.__atts = grads.transpose()\n",
    "                self.__normalised = normalised_grads.transpose()\n",
    "                first_run = False\n",
    "            else:\n",
    "                self.__atts = [self.__atts, grads.transpose()]\n",
    "                self.__normalised = [self.__normalised, normalised_grads.transpose()]\n",
    "                        \n",
    "        return self.__atts#, self.__normalised\n",
    "    \n",
    "class Bootstrapper():\n",
    "    def __init__(self, model, data, features, labels, suffix, explainer_type, num_straps = 50, back_size = 100):\n",
    "        self.explainer_type = explainer_type\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.num_straps = num_straps\n",
    "        self.back_size = back_size\n",
    "        self.suffix = suffix\n",
    "        \n",
    "    def bootstrap(self, X):\n",
    "        self.values = np.empty((self.num_straps, len(self.labels), len(self.features)))\n",
    "        self.mean_std_arr = np.empty((2, len(self.labels), len(self.features)))\n",
    "        for i in range(self.num_straps):\n",
    "            background_i = shap.sample(self.data, self.back_size, random_state = np.random.randint(100))\n",
    "            if self.explainer_type == 'kernel':\n",
    "                exp_i = shap.KernelExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'sample':\n",
    "                exp_i = shap.SampleExplainer(self.model, background_i)\n",
    "                shapper = exp_i.shap_values(X)\n",
    "            elif self.explainer_type == 'lime':\n",
    "                exp_i = MyLime(self.model, background_i, mode=\"regression\")\n",
    "                shapper = exp_i.attributions(X)\n",
    "            self.values[i,0,:] = shapper[0]\n",
    "            self.values[i,1,:] = shapper[1]\n",
    "        for i in range(len(self.labels)):\n",
    "            for j in range(len(self.features)):\n",
    "                self.mean_std_arr[0, i, j] = np.mean(self.values[:,i,j])\n",
    "                self.mean_std_arr[1, i, j] = np.std(self.values[:,i,j])\n",
    "            \n",
    "        return self.mean_std_arr\n",
    "    \n",
    "    def to_df(self):\n",
    "        self.bootstrap_df = self.x_list.copy()\n",
    "        for k, col in enumerate([\"mean\", \"std\"]):\n",
    "            for j in range(len(self.labels)):\n",
    "                for i in range(len(self.features)):\n",
    "                    self.bootstrap_df.insert(4 + i + j*len(self.features) + k*len(self.features)*len(self.labels), \n",
    "                                             self.features[i] + \"_\" + self.labels[j] + \"_\" + col, \n",
    "                                             self.bootstrap_array[:,k,j,i])\n",
    "        if self.save:\n",
    "            self.bootstrap_df.to_csv(\"Results/\"+self.explainer_type+\"/\"+self.explainer_type+\"bootstrap_vals_\"+self.suffix+\".csv\")\n",
    "        return self.bootstrap_df\n",
    "    def calculate(self, num_samples = 10, save = True):\n",
    "        self.x_list = self.data.iloc[np.sort(np.random.choice(self.data.shape[0], num_samples, replace =False))]\n",
    "        self.bootstrap_array = np.empty((num_samples, 2, len(self.labels), len(self.features)))\n",
    "        for i in tqdm(range(self.x_list.shape[0]), desc=\"Bootstrapping…\", ascii=False, ncols=75):\n",
    "            x_val = self.x_list.iloc[i,:]\n",
    "            self.bootstrap_array[i,:,:,:] = self.bootstrap(x_val)\n",
    "        return self.to_df()\n",
    "    \n",
    "    \n",
    "class MyLime(shap.other.LimeTabular):\n",
    "    def __init__(self, model, data, mode=\"classification\"):\n",
    "        self.model = model\n",
    "        assert mode in [\"classification\", \"regression\"]\n",
    "        self.mode = mode\n",
    "\n",
    "        if str(type(data)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            data = data.values\n",
    "        self.data = data\n",
    "        self.explainer = lime.lime_tabular.LimeTabularExplainer(data, mode=mode)\n",
    "        self.out_dim = 1#self.model(data[0:1]).shape[1]\n",
    "            \n",
    "    def attributions(self, X, num_samples=500, num_features=None):\n",
    "        try:\n",
    "            num_features = X.shape[1] if num_features is None else num_features\n",
    "        except:\n",
    "            print('exception')\n",
    "            num_features = 1\n",
    "        if str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n",
    "            X = X.values\n",
    "            \n",
    "        out = [np.zeros(X.shape) for j in range(len(self.model))]\n",
    "        for i in tqdm(range(X.shape[0]), desc=\"Calculating Lime…\", ascii=False, ncols=75):\n",
    "            exp1 = self.explainer.explain_instance(X[i], self.model[0], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            exp2 = self.explainer.explain_instance(X[i], self.model[1], labels=range(self.out_dim), \n",
    "                                                    num_features=num_features, num_samples=num_samples)\n",
    "            for k, v in exp1.local_exp[1]: \n",
    "                out[0][i,k] = v\n",
    "            for k, v in exp2.local_exp[1]: \n",
    "                out[1][i,k] = v\n",
    "          \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Define and Create Model\n",
    "\"\"\"\n",
    "def MLModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='sigmoid'),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "  \n",
    "    \n",
    "def SimpleModel():\n",
    "    opt = Adam(learning_rate=0.001, beta_1=0.7)\n",
    "    loss='mse'\n",
    "    model = Sequential([\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(2)            \n",
    "    ])\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "parameter_list = [{'alpha' : 1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.37, 'delta' : 1.0, 'omega' : 1.2}, \n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.5, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : -1.0, 'beta' : 1.0, 'gamma' : 0.0, 'delta' : 0.3, 'omega' : 0.0},\n",
    "                  {'alpha' : -1.0, 'beta' : -1.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 0.0, 'beta' : 0.0, 'gamma' : 0.37, 'delta' : 0.3, 'omega' : 1.2},\n",
    "                  {'alpha' : 1.0, 'beta' : -0.5, 'gamma' : 0.37, 'delta' : 0.0, 'omega' : 1.2}]\n",
    "\n",
    "\"\"\"\n",
    "Define Parameter Configuration to Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, linear stiffness\n",
    "    beta  : float, non linearity in the restoring force\n",
    "    gamma : float, amplitude of the periodic driving force\n",
    "    delta : float, amount of damping\n",
    "    omega : float, angular frequency of the periodic driving force\n",
    "\"\"\"   \n",
    "for dict_param in parameter_list:\n",
    "    duffing = Duffing(parameters = dict_param)\n",
    "    eom = duffing.eom\n",
    "    suffix = duffing.suffix\n",
    "\n",
    "    end_time = 100\n",
    "    duffing.generate(100, samples = 10, end_time = end_time)\n",
    "    duffing.scale_features()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(duffing.X_df[duffing.features], duffing.X_df[duffing.labels], test_size=0.1, random_state=42)\n",
    "    \n",
    "    X = X_test\n",
    "    y = y_test\n",
    "    \n",
    "    # Create a basic model instance\n",
    "    model = MLModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25),tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)]\n",
    "\n",
    "\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history=model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    model.save(\"Models/ml_model_\"+suffix)\n",
    "    with open('Models/TrainingHistory/'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    def ml_x(X):\n",
    "        return model.predict(X)[:,0]\n",
    "    def ml_v(X):\n",
    "        return model.predict(X)[:,1]\n",
    "    \n",
    "    # Make Simple ML Model\n",
    "    simple_model = SimpleModel()\n",
    "\n",
    "    \"\"\"\n",
    "    Train Model\n",
    "    \"\"\"\n",
    "    # pipe = make_pipeline(scaler, model)\n",
    "\n",
    "    history_simple=simple_model.fit(X_train, y_train, steps_per_epoch=None, epochs=5, validation_split=0.2, batch_size=1024, shuffle=True, callbacks=callbacks, verbose=0)\n",
    "    with open('Models/TrainingHistory/simple_'+suffix, 'wb') as file_pi:\n",
    "        pickle.dump(history_simple.history, file_pi)\n",
    "    loss = simple_model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"Trained model, loss: {:5.2f}%\".format(loss))\n",
    "\n",
    "    simple_model.save(\"Models/simple_ml_model_\"+suffix)\n",
    "    \n",
    "    def simple_ml_x(X):\n",
    "        return simple_model.predict(X)[:,0]\n",
    "    def simple_ml_v(X):\n",
    "        return simple_model.predict(X)[:,1]\n",
    "\n",
    "    explainers = ['lime']#[\"kernel\", \"sampling\", \"lime\", \"numeric\"]\n",
    "    true_lime = [duffing.predict_x, duffing.predict_v]\n",
    "    ml_lime = [ml_x, ml_v]\n",
    "    simple_lime = [simple_ml_x, simple_ml_v]\n",
    "    models = {\"true\" : duffing.predict, \"ml\" : model.predict, \"simple\" : simple_model.predict}\n",
    "    lime_models = {\"true\" : true_lime, \"ml\" : ml_lime, \"simple\" : simple_lime}\n",
    "\n",
    "    background = shap.sample(X_test, 10)\n",
    "    choice = X.iloc[np.sort(np.random.choice(X_test.shape[0], 10, replace =False))]\n",
    "\n",
    "\n",
    "    big_df = pd.DataFrame()\n",
    "    for model_ in models:\n",
    "        for explainer in explainers:\n",
    "            print(explainer + model_)\n",
    "            if explainer == \"kernel\":\n",
    "                temp_explainer = shap.KernelExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"sampling\":\n",
    "                temp_explainer = shap.SamplingExplainer(models[model_], background)\n",
    "                temp_vals = temp_explainer.shap_values(choice)\n",
    "            elif explainer == \"lime\":\n",
    "                temp_explainer = MyLime(lime_models[model_], choice, mode='regression')\n",
    "                temp_vals = temp_explainer.attributions(choice)\n",
    "            elif explainer == \"numeric\":\n",
    "                temp_explainer = NumericExplainer(models[model_], duffing.features, duffing.labels, h = 0.001)\n",
    "                temp_vals = temp_explainer.feature_att(choice)\n",
    "            else:\n",
    "                print(\"not a valid explainer type\")\n",
    "            big_df = big_df.append(duffing.vals_to_df(temp_vals, \n",
    "                                                            choice, explainer = explainer, suffix = suffix))\n",
    "\n",
    "        \n",
    "    big_df.to_csv(\"Results/explainer_dataframe_\"+suffix+\".csv\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd8f63e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28947776, -0.99137015])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_lime[0](choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20be5afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>v0</th>\n",
       "      <th>t</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0.723507</td>\n",
       "      <td>0.296005</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.109136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.500810</td>\n",
       "      <td>0.565427</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.003995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0.075147</td>\n",
       "      <td>0.293592</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.503701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.236473</td>\n",
       "      <td>0.242933</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.176064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.698765</td>\n",
       "      <td>0.708605</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.099371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.918358</td>\n",
       "      <td>0.849232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.417772</td>\n",
       "      <td>0.883164</td>\n",
       "      <td>0.120482</td>\n",
       "      <td>0.161473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.275376</td>\n",
       "      <td>0.554443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.236473</td>\n",
       "      <td>0.242933</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.176064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.204524</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.084286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x0        v0         t    energy\n",
       "737  0.723507  0.296005  0.843373  0.109136\n",
       "811  0.500810  0.565427  0.120482  0.003995\n",
       "883  0.075147  0.293592  0.469880  0.503701\n",
       "101  0.236473  0.242933  0.120482  0.176064\n",
       "346  0.698765  0.708605  0.638554  0.099371\n",
       "30   0.918358  0.849232  0.000000  0.560406\n",
       "221  0.417772  0.883164  0.120482  0.161473\n",
       "70   0.275376  0.554443  0.000000  0.077855\n",
       "107  0.236473  0.242933  0.843373  0.176064\n",
       "218  0.457968  0.204524  0.963855  0.084286"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a10d52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28947776, -0.48052421, -0.13659773, -0.62441735,  0.8743461 ,\n",
       "        1.6598387 ,  0.09658614, -0.90111283,  0.28938734,  0.70700482])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duffing.predict(choice)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d5f2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28947776, -0.99137015],\n",
       "       [-0.48052421,  0.81858931],\n",
       "       [-0.13659773, -1.02911077],\n",
       "       [-0.62441735,  0.51833618],\n",
       "       [ 0.8743461 ,  0.39192726],\n",
       "       [ 1.6598387 ,  1.40015431],\n",
       "       [ 0.09658614,  1.05512528],\n",
       "       [-0.90111283,  0.2449737 ],\n",
       "       [ 0.28938734, -0.99141582],\n",
       "       [ 0.70700482, -0.72648899]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duffing.predict(choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acddc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
