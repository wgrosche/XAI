{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for explaining classifiers that use tabular data (matrices).\n",
    "\"\"\"\n",
    "import collections\n",
    "import copy\n",
    "from functools import partial\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "from pyDOE2 import lhs\n",
    "from scipy.stats.distributions import norm\n",
    "\n",
    "from lime.discretize import QuartileDiscretizer\n",
    "from lime.discretize import DecileDiscretizer\n",
    "from lime.discretize import EntropyDiscretizer\n",
    "from lime.discretize import BaseDiscretizer\n",
    "from lime.discretize import StatsDiscretizer\n",
    "from . import explanation\n",
    "from . import lime_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDomainMapper(explanation.DomainMapper):\n",
    "    \"\"\"Maps feature ids to names, generates table views, etc\"\"\"\n",
    "\n",
    "    def __init__(self, feature_names, feature_values, scaled_row,\n",
    "                 categorical_features, discretized_feature_names=None,\n",
    "                 feature_indexes=None):\n",
    "        \"\"\"Init.\n",
    "\n",
    "        Args:\n",
    "            feature_names: list of feature names, in order\n",
    "            feature_values: list of strings with the values of the original row\n",
    "            scaled_row: scaled row\n",
    "            categorical_features: list of categorical features ids (ints)\n",
    "            feature_indexes: optional feature indexes used in the sparse case\n",
    "        \"\"\"\n",
    "        self.exp_feature_names = feature_names\n",
    "        self.discretized_feature_names = discretized_feature_names\n",
    "        self.feature_names = feature_names\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_indexes = feature_indexes\n",
    "        self.scaled_row = scaled_row\n",
    "        if sp.sparse.issparse(scaled_row):\n",
    "            self.all_categorical = False\n",
    "        else:\n",
    "            self.all_categorical = len(categorical_features) == len(scaled_row)\n",
    "        self.categorical_features = categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d440e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def map_exp_ids(self, exp):\n",
    "        \"\"\"Maps ids to feature names.\n",
    "\n",
    "        Args:\n",
    "            exp: list of tuples [(id, weight), (id,weight)]\n",
    "\n",
    "        Returns:\n",
    "            list of tuples (feature_name, weight)\n",
    "        \"\"\"\n",
    "        names = self.exp_feature_names\n",
    "        if self.discretized_feature_names is not None:\n",
    "            names = self.discretized_feature_names\n",
    "        return [(names[x[0]], x[1]) for x in exp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b91ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_instance_html(self,\n",
    "                                exp,\n",
    "                                label,\n",
    "                                div_name,\n",
    "                                exp_object_name,\n",
    "                                show_table=True,\n",
    "                                show_all=False):\n",
    "        \"\"\"Shows the current example in a table format.\n",
    "\n",
    "        Args:\n",
    "             exp: list of tuples [(id, weight), (id,weight)]\n",
    "             label: label id (integer)\n",
    "             div_name: name of div object to be used for rendering(in js)\n",
    "             exp_object_name: name of js explanation object\n",
    "             show_table: if False, don't show table visualization.\n",
    "             show_all: if True, show zero-weighted features in the table.\n",
    "        \"\"\"\n",
    "        if not show_table:\n",
    "            return ''\n",
    "        weights = [0] * len(self.feature_names)\n",
    "        for x in exp:\n",
    "            weights[x[0]] = x[1]\n",
    "        if self.feature_indexes is not None:\n",
    "            # Sparse case: only display the non-zero values and importances\n",
    "            fnames = [self.exp_feature_names[i] for i in self.feature_indexes]\n",
    "            fweights = [weights[i] for i in self.feature_indexes]\n",
    "            if show_all:\n",
    "                out_list = list(zip(fnames,\n",
    "                                    self.feature_values,\n",
    "                                    fweights))\n",
    "            else:\n",
    "                out_dict = dict(map(lambda x: (x[0], (x[1], x[2], x[3])),\n",
    "                                zip(self.feature_indexes,\n",
    "                                    fnames,\n",
    "                                    self.feature_values,\n",
    "                                    fweights)))\n",
    "                out_list = [out_dict.get(x[0], (str(x[0]), 0.0, 0.0)) for x in exp]\n",
    "        else:\n",
    "            out_list = list(zip(self.exp_feature_names,\n",
    "                                self.feature_values,\n",
    "                                weights))\n",
    "            if not show_all:\n",
    "                out_list = [out_list[x[0]] for x in exp]\n",
    "        ret = u'''\n",
    "            %s.show_raw_tabular(%s, %d, %s);\n",
    "        ''' % (exp_object_name, json.dumps(out_list, ensure_ascii=False), label, div_name)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimeTabularExplainer(object):\n",
    "    \"\"\"Explains predictions on tabular (i.e. matrix) data.\n",
    "    For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "    doing the inverse operation of mean-centering and scaling, according to the\n",
    "    means and stds in the training data. For categorical features, perturb by\n",
    "    sampling according to the training distribution, and making a binary\n",
    "    feature that is 1 when the value is the same as the instance being\n",
    "    explained.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d93ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self,\n",
    "                 training_data,\n",
    "                 mode=\"classification\",\n",
    "                 training_labels=None,\n",
    "                 feature_names=None,\n",
    "                 categorical_features=None,\n",
    "                 categorical_names=None,\n",
    "                 kernel_width=None,\n",
    "                 kernel=None,\n",
    "                 verbose=False,\n",
    "                 class_names=None,\n",
    "                 feature_selection='auto',\n",
    "                 discretize_continuous=True,\n",
    "                 discretizer='quartile',\n",
    "                 sample_around_instance=False,\n",
    "                 random_state=None,\n",
    "                 training_data_stats=None):\n",
    "        \"\"\"Init function.\n",
    "\n",
    "        Args:\n",
    "            training_data: numpy 2d array\n",
    "            mode: \"classification\" or \"regression\"\n",
    "            training_labels: labels for training data. Not required, but may be\n",
    "                used by discretizer.\n",
    "            feature_names: list of names (strings) corresponding to the columns\n",
    "                in the training data.\n",
    "            categorical_features: list of indices (ints) corresponding to the\n",
    "                categorical columns. Everything else will be considered\n",
    "                continuous. Values in these columns MUST be integers.\n",
    "            categorical_names: map from int to list of names, where\n",
    "                categorical_names[x][y] represents the name of the yth value of\n",
    "                column x.\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "                If None, defaults to sqrt (number of columns) * 0.75\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            discretize_continuous: if True, all non-categorical features will\n",
    "                be discretized into quartiles.\n",
    "            discretizer: only matters if discretize_continuous is True\n",
    "                and data is not sparse. Options are 'quartile', 'decile',\n",
    "                'entropy' or a BaseDiscretizer instance.\n",
    "            sample_around_instance: if True, will sample continuous features\n",
    "                in perturbed samples from a normal centered at the instance\n",
    "                being explained. Otherwise, the normal is centered on the mean\n",
    "                of the feature data.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "            training_data_stats: a dict object having the details of training data\n",
    "                statistics. If None, training data information will be used, only matters\n",
    "                if discretize_continuous is True. Must have the following keys:\n",
    "                means\", \"mins\", \"maxs\", \"stds\", \"feature_values\",\n",
    "                \"feature_frequencies\"\n",
    "        \"\"\"\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.mode = mode\n",
    "        self.categorical_names = categorical_names or {}\n",
    "        self.sample_around_instance = sample_around_instance\n",
    "        self.training_data_stats = training_data_stats\n",
    "\n",
    "        # Check and raise proper error in stats are supplied in non-descritized path\n",
    "        if self.training_data_stats:\n",
    "            self.validate_training_data_stats(self.training_data_stats)\n",
    "\n",
    "        if categorical_features is None:\n",
    "            categorical_features = []\n",
    "        if feature_names is None:\n",
    "            feature_names = [str(i) for i in range(training_data.shape[1])]\n",
    "\n",
    "        self.categorical_features = list(categorical_features)\n",
    "        self.feature_names = list(feature_names)\n",
    "\n",
    "        self.discretizer = None\n",
    "        if discretize_continuous and not sp.sparse.issparse(training_data):\n",
    "            # Set the discretizer if training data stats are provided\n",
    "            if self.training_data_stats:\n",
    "                discretizer = StatsDiscretizer(\n",
    "                    training_data, self.categorical_features,\n",
    "                    self.feature_names, labels=training_labels,\n",
    "                    data_stats=self.training_data_stats,\n",
    "                    random_state=self.random_state)\n",
    "\n",
    "            if discretizer == 'quartile':\n",
    "                self.discretizer = QuartileDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels,\n",
    "                        random_state=self.random_state)\n",
    "            elif discretizer == 'decile':\n",
    "                self.discretizer = DecileDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels,\n",
    "                        random_state=self.random_state)\n",
    "            elif discretizer == 'entropy':\n",
    "                self.discretizer = EntropyDiscretizer(\n",
    "                        training_data, self.categorical_features,\n",
    "                        self.feature_names, labels=training_labels,\n",
    "                        random_state=self.random_state)\n",
    "            elif isinstance(discretizer, BaseDiscretizer):\n",
    "                self.discretizer = discretizer\n",
    "            else:\n",
    "                raise ValueError('''Discretizer must be 'quartile',''' +\n",
    "                                 ''' 'decile', 'entropy' or a''' +\n",
    "                                 ''' BaseDiscretizer instance''')\n",
    "            self.categorical_features = list(range(training_data.shape[1]))\n",
    "\n",
    "            # Get the discretized_training_data when the stats are not provided\n",
    "            if(self.training_data_stats is None):\n",
    "                discretized_training_data = self.discretizer.discretize(\n",
    "                    training_data)\n",
    "\n",
    "        if kernel_width is None:\n",
    "            kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
    "        kernel_width = float(kernel_width)\n",
    "\n",
    "        if kernel is None:\n",
    "            def kernel(d, kernel_width):\n",
    "                return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "\n",
    "        self.feature_selection = feature_selection\n",
    "        self.base = lime_base.LimeBase(kernel_fn, verbose, random_state=self.random_state)\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # Though set has no role to play if training data stats are provided\n",
    "        self.scaler = sklearn.preprocessing.StandardScaler(with_mean=False)\n",
    "        self.scaler.fit(training_data)\n",
    "        self.feature_values = {}\n",
    "        self.feature_frequencies = {}\n",
    "\n",
    "        for feature in self.categorical_features:\n",
    "            if training_data_stats is None:\n",
    "                if self.discretizer is not None:\n",
    "                    column = discretized_training_data[:, feature]\n",
    "                else:\n",
    "                    column = training_data[:, feature]\n",
    "\n",
    "                feature_count = collections.Counter(column)\n",
    "                values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n",
    "            else:\n",
    "                values = training_data_stats[\"feature_values\"][feature]\n",
    "                frequencies = training_data_stats[\"feature_frequencies\"][feature]\n",
    "\n",
    "            self.feature_values[feature] = values\n",
    "            self.feature_frequencies[feature] = (np.array(frequencies) /\n",
    "                                                 float(sum(frequencies)))\n",
    "            self.scaler.mean_[feature] = 0\n",
    "            self.scaler.scale_[feature] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def explain_instance(self,\n",
    "                         data_row,\n",
    "                         predict_fn,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=10,\n",
    "                         num_samples=5000,\n",
    "                         distance_metric='euclidean',\n",
    "                         model_regressor=None,\n",
    "                         sampling_method='gaussian'):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "\n",
    "        First, we generate neighborhood data by randomly perturbing features\n",
    "        from the instance (see __data_inverse). We then learn locally weighted\n",
    "        linear models on this neighborhood data to explain each of the classes\n",
    "        in an interpretable way (see lime_base.py).\n",
    "\n",
    "        Args:\n",
    "            data_row: 1d numpy array or scipy.sparse matrix, corresponding to a row\n",
    "            predict_fn: prediction function. For classifiers, this should be a\n",
    "                function that takes a numpy array and outputs prediction\n",
    "                probabilities. For regressors, this takes a numpy array and\n",
    "                returns the predictions. For ScikitClassifiers, this is\n",
    "                `classifier.predict_proba()`. For ScikitRegressors, this\n",
    "                is `regressor.predict()`. The prediction function needs to work\n",
    "                on multiple feature vectors (the vectors randomly perturbed\n",
    "                from the data_row).\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for weights.\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "                to Ridge regression in LimeBase. Must have model_regressor.coef_\n",
    "                and 'sample_weight' as a parameter to model_regressor.fit()\n",
    "            sampling_method: Method to sample synthetic data. Defaults to Gaussian\n",
    "                sampling. Can also use Latin Hypercube Sampling.\n",
    "\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "        data, inverse = self.__data_inverse(data_row, num_samples, sampling_method)\n",
    "\n",
    "        scaled_data = (data - self.scaler.mean_) / self.scaler.scale_\n",
    "        distances = sklearn.metrics.pairwise_distances(\n",
    "                scaled_data,\n",
    "                scaled_data[0].reshape(1, -1),\n",
    "                metric=distance_metric\n",
    "        ).ravel()\n",
    "\n",
    "        yss = predict_fn(inverse)\n",
    "\n",
    "        # for regression, the output should be a one-dimensional array of predictions\n",
    "        else:\n",
    "            try:\n",
    "                if len(yss.shape) != 1 and len(yss[0].shape) == 1:\n",
    "                    yss = np.array([v[0] for v in yss])\n",
    "                assert isinstance(yss, np.ndarray) and len(yss.shape) == 1\n",
    "            except AssertionError:\n",
    "                raise ValueError(\"Your model needs to output single-dimensional \\\n",
    "                    numpyarrays, not arrays of {} dimensions\".format(yss.shape))\n",
    "\n",
    "            predicted_value = yss[0]\n",
    "            min_y = min(yss)\n",
    "            max_y = max(yss)\n",
    "\n",
    "            # add a dimension to be compatible with downstream machinery\n",
    "            yss = yss[:, np.newaxis]\n",
    "\n",
    "        feature_names = copy.deepcopy(self.feature_names)\n",
    "        if feature_names is None:\n",
    "            feature_names = [str(x) for x in range(data_row.shape[0])]\n",
    "\n",
    "        if sp.sparse.issparse(data_row):\n",
    "            values = self.convert_and_round(data_row.data)\n",
    "            feature_indexes = data_row.indices\n",
    "        else:\n",
    "            values = self.convert_and_round(data_row)\n",
    "            feature_indexes = None\n",
    "\n",
    "        for i in self.categorical_features:\n",
    "            if self.discretizer is not None and i in self.discretizer.lambdas:\n",
    "                continue\n",
    "            name = int(data_row[i])\n",
    "            if i in self.categorical_names:\n",
    "                name = self.categorical_names[i][name]\n",
    "            feature_names[i] = '%s=%s' % (feature_names[i], name)\n",
    "            values[i] = 'True'\n",
    "        categorical_features = self.categorical_features\n",
    "\n",
    "        discretized_feature_names = None\n",
    "        if self.discretizer is not None:\n",
    "            categorical_features = range(data.shape[1])\n",
    "            discretized_instance = self.discretizer.discretize(data_row)\n",
    "            discretized_feature_names = copy.deepcopy(feature_names)\n",
    "            for f in self.discretizer.names:\n",
    "                discretized_feature_names[f] = self.discretizer.names[f][int(\n",
    "                        discretized_instance[f])]\n",
    "\n",
    "        domain_mapper = TableDomainMapper(feature_names,\n",
    "                                          values,\n",
    "                                          scaled_data[0],\n",
    "                                          categorical_features=categorical_features,\n",
    "                                          discretized_feature_names=discretized_feature_names,\n",
    "                                          feature_indexes=feature_indexes)\n",
    "        ret_exp = explanation.Explanation(domain_mapper,\n",
    "                                          mode=self.mode,\n",
    "                                          class_names=self.class_names)\n",
    "        if self.mode == \"classification\":\n",
    "            ret_exp.predict_proba = yss[0]\n",
    "            if top_labels:\n",
    "                labels = np.argsort(yss[0])[-top_labels:]\n",
    "                ret_exp.top_labels = list(labels)\n",
    "                ret_exp.top_labels.reverse()\n",
    "        else:\n",
    "            ret_exp.predicted_value = predicted_value\n",
    "            ret_exp.min_value = min_y\n",
    "            ret_exp.max_value = max_y\n",
    "            labels = [0]\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score[label],\n",
    "             ret_exp.local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                    scaled_data,\n",
    "                    yss,\n",
    "                    distances,\n",
    "                    label,\n",
    "                    num_features,\n",
    "                    model_regressor=model_regressor,\n",
    "                    feature_selection=self.feature_selection)\n",
    "\n",
    "        if self.mode == \"regression\":\n",
    "            ret_exp.intercept[1] = ret_exp.intercept[0]\n",
    "            ret_exp.local_exp[1] = [x for x in ret_exp.local_exp[0]]\n",
    "            ret_exp.local_exp[0] = [(i, -1 * j) for i, j in ret_exp.local_exp[1]]\n",
    "\n",
    "        return ret_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    @staticmethod\n",
    "    def convert_and_round(values):\n",
    "        return ['%.2f' % v for v in values]\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_training_data_stats(training_data_stats):\n",
    "        \"\"\"\n",
    "            Method to validate the structure of training data stats\n",
    "        \"\"\"\n",
    "        stat_keys = list(training_data_stats.keys())\n",
    "        valid_stat_keys = [\"means\", \"mins\", \"maxs\", \"stds\", \"feature_values\", \"feature_frequencies\"]\n",
    "        missing_keys = list(set(valid_stat_keys) - set(stat_keys))\n",
    "        if len(missing_keys) > 0:\n",
    "            raise Exception(\"Missing keys in training_data_stats. Details: %s\" % (missing_keys))\n",
    "\n",
    "\n",
    "\n",
    "    def __data_inverse(self,\n",
    "                       data_row,\n",
    "                       num_samples,\n",
    "                       sampling_method):\n",
    "        \"\"\"Generates a neighborhood around a prediction.\n",
    "\n",
    "        For numerical features, perturb them by sampling from a Normal(0,1) and\n",
    "        doing the inverse operation of mean-centering and scaling, according to\n",
    "        the means and stds in the training data. For categorical features,\n",
    "        perturb by sampling according to the training distribution, and making\n",
    "        a binary feature that is 1 when the value is the same as the instance\n",
    "        being explained.\n",
    "\n",
    "        Args:\n",
    "            data_row: 1d numpy array, corresponding to a row\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            sampling_method: 'gaussian' or 'lhs'\n",
    "\n",
    "        Returns:\n",
    "            A tuple (data, inverse), where:\n",
    "                data: dense num_samples * K matrix, where categorical features\n",
    "                are encoded with either 0 (not equal to the corresponding value\n",
    "                in data_row) or 1. The first row is the original instance.\n",
    "                inverse: same as data, except the categorical features are not\n",
    "                binary, but categorical (as the original data)\n",
    "        \"\"\"\n",
    "        is_sparse = sp.sparse.issparse(data_row)\n",
    "        if is_sparse:\n",
    "            num_cols = data_row.shape[1]\n",
    "            data = sp.sparse.csr_matrix((num_samples, num_cols), dtype=data_row.dtype)\n",
    "        else:\n",
    "            num_cols = data_row.shape[0]\n",
    "            data = np.zeros((num_samples, num_cols))\n",
    "        categorical_features = range(num_cols)\n",
    "        if self.discretizer is None:\n",
    "            instance_sample = data_row\n",
    "            scale = self.scaler.scale_\n",
    "            mean = self.scaler.mean_\n",
    "            if is_sparse:\n",
    "                # Perturb only the non-zero values\n",
    "                non_zero_indexes = data_row.nonzero()[1]\n",
    "                num_cols = len(non_zero_indexes)\n",
    "                instance_sample = data_row[:, non_zero_indexes]\n",
    "                scale = scale[non_zero_indexes]\n",
    "                mean = mean[non_zero_indexes]\n",
    "\n",
    "            if sampling_method == 'gaussian':\n",
    "                data = self.random_state.normal(0, 1, num_samples * num_cols\n",
    "                                                ).reshape(num_samples, num_cols)\n",
    "                data = np.array(data)\n",
    "            elif sampling_method == 'lhs':\n",
    "                data = lhs(num_cols, samples=num_samples\n",
    "                           ).reshape(num_samples, num_cols)\n",
    "                means = np.zeros(num_cols)\n",
    "                stdvs = np.array([1]*num_cols)\n",
    "                for i in range(num_cols):\n",
    "                    data[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(data[:, i])\n",
    "                data = np.array(data)\n",
    "            else:\n",
    "                warnings.warn('''Invalid input for sampling_method.\n",
    "                                 Defaulting to Gaussian sampling.''', UserWarning)\n",
    "                data = self.random_state.normal(0, 1, num_samples * num_cols\n",
    "                                                ).reshape(num_samples, num_cols)\n",
    "                data = np.array(data)\n",
    "\n",
    "            if self.sample_around_instance:\n",
    "                data = data * scale + instance_sample\n",
    "            else:\n",
    "                data = data * scale + mean\n",
    "            if is_sparse:\n",
    "                if num_cols == 0:\n",
    "                    data = sp.sparse.csr_matrix((num_samples,\n",
    "                                                 data_row.shape[1]),\n",
    "                                                dtype=data_row.dtype)\n",
    "                else:\n",
    "                    indexes = np.tile(non_zero_indexes, num_samples)\n",
    "                    indptr = np.array(\n",
    "                        range(0, len(non_zero_indexes) * (num_samples + 1),\n",
    "                              len(non_zero_indexes)))\n",
    "                    data_1d_shape = data.shape[0] * data.shape[1]\n",
    "                    data_1d = data.reshape(data_1d_shape)\n",
    "                    data = sp.sparse.csr_matrix(\n",
    "                        (data_1d, indexes, indptr),\n",
    "                        shape=(num_samples, data_row.shape[1]))\n",
    "            categorical_features = self.categorical_features\n",
    "            first_row = data_row\n",
    "        else:\n",
    "            first_row = self.discretizer.discretize(data_row)\n",
    "        data[0] = data_row.copy()\n",
    "        inverse = data.copy()\n",
    "        for column in categorical_features:\n",
    "            values = self.feature_values[column]\n",
    "            freqs = self.feature_frequencies[column]\n",
    "            inverse_column = self.random_state.choice(values, size=num_samples,\n",
    "                                                      replace=True, p=freqs)\n",
    "            binary_column = (inverse_column == first_row[column]).astype(int)\n",
    "            binary_column[0] = 1\n",
    "            inverse_column[0] = data[0, column]\n",
    "            data[:, column] = binary_column\n",
    "            inverse[:, column] = inverse_column\n",
    "        if self.discretizer is not None:\n",
    "            inverse[1:] = self.discretizer.undiscretize(inverse[1:])\n",
    "        inverse[0] = data_row\n",
    "        return data, inverse\n",
    "\n",
    "\n",
    "class RecurrentTabularExplainer(LimeTabularExplainer):\n",
    "    \"\"\"\n",
    "    An explainer for keras-style recurrent neural networks, where the\n",
    "    input shape is (n_samples, n_timesteps, n_features). This class\n",
    "    just extends the LimeTabularExplainer class and reshapes the training\n",
    "    data and feature names such that they become something like\n",
    "\n",
    "    (val1_t1, val1_t2, val1_t3, ..., val2_t1, ..., valn_tn)\n",
    "\n",
    "    Each of the methods that take data reshape it appropriately,\n",
    "    so you can pass in the training/testing data exactly as you\n",
    "    would to the recurrent neural network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, training_data, mode=\"classification\",\n",
    "                 training_labels=None, feature_names=None,\n",
    "                 categorical_features=None, categorical_names=None,\n",
    "                 kernel_width=None, kernel=None, verbose=False, class_names=None,\n",
    "                 feature_selection='auto', discretize_continuous=True,\n",
    "                 discretizer='quartile', random_state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            training_data: numpy 3d array with shape\n",
    "                (n_samples, n_timesteps, n_features)\n",
    "            mode: \"classification\" or \"regression\"\n",
    "            training_labels: labels for training data. Not required, but may be\n",
    "                used by discretizer.\n",
    "            feature_names: list of names (strings) corresponding to the columns\n",
    "                in the training data.\n",
    "            categorical_features: list of indices (ints) corresponding to the\n",
    "                categorical columns. Everything else will be considered\n",
    "                continuous. Values in these columns MUST be integers.\n",
    "            categorical_names: map from int to list of names, where\n",
    "                categorical_names[x][y] represents the name of the yth value of\n",
    "                column x.\n",
    "            kernel_width: kernel width for the exponential kernel.\n",
    "            If None, defaults to sqrt(number of columns) * 0.75\n",
    "            kernel: similarity kernel that takes euclidean distances and kernel\n",
    "                width as input and outputs weights in (0,1). If None, defaults to\n",
    "                an exponential kernel.\n",
    "            verbose: if true, print local prediction values from linear model\n",
    "            class_names: list of class names, ordered according to whatever the\n",
    "                classifier is using. If not present, class names will be '0',\n",
    "                '1', ...\n",
    "            feature_selection: feature selection method. can be\n",
    "                'forward_selection', 'lasso_path', 'none' or 'auto'.\n",
    "                See function 'explain_instance_with_data' in lime_base.py for\n",
    "                details on what each of the options does.\n",
    "            discretize_continuous: if True, all non-categorical features will\n",
    "                be discretized into quartiles.\n",
    "            discretizer: only matters if discretize_continuous is True. Options\n",
    "                are 'quartile', 'decile', 'entropy' or a BaseDiscretizer\n",
    "                instance.\n",
    "            random_state: an integer or numpy.RandomState that will be used to\n",
    "                generate random numbers. If None, the random state will be\n",
    "                initialized using the internal numpy seed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Reshape X\n",
    "        n_samples, n_timesteps, n_features = training_data.shape\n",
    "        training_data = np.transpose(training_data, axes=(0, 2, 1)).reshape(\n",
    "                n_samples, n_timesteps * n_features)\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        if feature_names is None:\n",
    "            feature_names = ['feature%d' % i for i in range(n_features)]\n",
    "\n",
    "        # Update the feature names\n",
    "        feature_names = ['{}_t-{}'.format(n, n_timesteps - (i + 1))\n",
    "                         for n in feature_names for i in range(n_timesteps)]\n",
    "\n",
    "        # Send off the the super class to do its magic.\n",
    "        super(RecurrentTabularExplainer, self).__init__(\n",
    "                training_data,\n",
    "                mode=mode,\n",
    "                training_labels=training_labels,\n",
    "                feature_names=feature_names,\n",
    "                categorical_features=categorical_features,\n",
    "                categorical_names=categorical_names,\n",
    "                kernel_width=kernel_width,\n",
    "                kernel=kernel,\n",
    "                verbose=verbose,\n",
    "                class_names=class_names,\n",
    "                feature_selection=feature_selection,\n",
    "                discretize_continuous=discretize_continuous,\n",
    "                discretizer=discretizer,\n",
    "                random_state=random_state)\n",
    "\n",
    "    def _make_predict_proba(self, func):\n",
    "        \"\"\"\n",
    "        The predict_proba method will expect 3d arrays, but we are reshaping\n",
    "        them to 2D so that LIME works correctly. This wraps the function\n",
    "        you give in explain_instance to first reshape the data to have\n",
    "        the shape the the keras-style network expects.\n",
    "        \"\"\"\n",
    "\n",
    "        def predict_proba(X):\n",
    "            n_samples = X.shape[0]\n",
    "            new_shape = (n_samples, self.n_features, self.n_timesteps)\n",
    "            X = np.transpose(X.reshape(new_shape), axes=(0, 2, 1))\n",
    "            return func(X)\n",
    "\n",
    "        return predict_proba\n",
    "\n",
    "    def explain_instance(self, data_row, classifier_fn, labels=(1,),\n",
    "                         top_labels=None, num_features=10, num_samples=5000,\n",
    "                         distance_metric='euclidean', model_regressor=None):\n",
    "        \"\"\"Generates explanations for a prediction.\n",
    "\n",
    "        First, we generate neighborhood data by randomly perturbing features\n",
    "        from the instance (see __data_inverse). We then learn locally weighted\n",
    "        linear models on this neighborhood data to explain each of the classes\n",
    "        in an interpretable way (see lime_base.py).\n",
    "\n",
    "        Args:\n",
    "            data_row: 2d numpy array, corresponding to a row\n",
    "            classifier_fn: classifier prediction probability function, which\n",
    "                takes a numpy array and outputs prediction probabilities. For\n",
    "                ScikitClassifiers , this is classifier.predict_proba.\n",
    "            labels: iterable with labels to be explained.\n",
    "            top_labels: if not None, ignore labels and produce explanations for\n",
    "                the K labels with highest prediction probabilities, where K is\n",
    "                this parameter.\n",
    "            num_features: maximum number of features present in explanation\n",
    "            num_samples: size of the neighborhood to learn the linear model\n",
    "            distance_metric: the distance metric to use for weights.\n",
    "            model_regressor: sklearn regressor to use in explanation. Defaults\n",
    "                to Ridge regression in LimeBase. Must have\n",
    "                model_regressor.coef_ and 'sample_weight' as a parameter\n",
    "                to model_regressor.fit()\n",
    "\n",
    "        Returns:\n",
    "            An Explanation object (see explanation.py) with the corresponding\n",
    "            explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        # Flatten input so that the normal explainer can handle it\n",
    "        data_row = data_row.T.reshape(self.n_timesteps * self.n_features)\n",
    "\n",
    "        # Wrap the classifier to reshape input\n",
    "        classifier_fn = self._make_predict_proba(classifier_fn)\n",
    "        return super(RecurrentTabularExplainer, self).explain_instance(\n",
    "            data_row, classifier_fn,\n",
    "            labels=labels,\n",
    "            top_labels=top_labels,\n",
    "            num_features=num_features,\n",
    "            num_samples=num_samples,\n",
    "            distance_metric=distance_metric,\n",
    "            model_regressor=model_regressor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   def explain_instance_with_data(self,\n",
    "                                   neighborhood_data,\n",
    "                                   neighborhood_labels,\n",
    "                                   distances,\n",
    "                                   label,\n",
    "                                   num_features,\n",
    "                                   feature_selection='auto',\n",
    "                                   model_regressor=None):\n",
    "        \"\"\"Takes perturbed data, labels and distances, returns explanation.\n",
    "        Args:\n",
    "            neighborhood_data: perturbed data, 2d array. first element is\n",
    "                               assumed to be the original data point.\n",
    "            neighborhood_labels: corresponding perturbed labels. should have as\n",
    "                                 many columns as the number of possible labels.\n",
    "            distances: distances to original data point.\n",
    "            label: label for which we want an explanation\n",
    "            num_features: maximum number of features in explanation\n",
    "            feature_selection: how to select num_features. options are:\n",
    "                'forward_selection': iteratively add features to the model.\n",
    "                    This is costly when num_features is high\n",
    "                'highest_weights': selects the features that have the highest\n",
    "                    product of absolute weight * original data point when\n",
    "                    learning with all the features\n",
    "                'lasso_path': chooses features based on the lasso\n",
    "                    regularization path\n",
    "                'none': uses all features, ignores num_features\n",
    "                'auto': uses forward_selection if num_features <= 6, and\n",
    "                    'highest_weights' otherwise.\n",
    "            model_regressor: sklearn regressor to use in explanation.\n",
    "                Defaults to Ridge regression if None. Must have\n",
    "                model_regressor.coef_ and 'sample_weight' as a parameter\n",
    "                to model_regressor.fit()\n",
    "        Returns:\n",
    "            (intercept, exp, score, local_pred):\n",
    "            intercept is a float.\n",
    "            exp is a sorted list of tuples, where each tuple (x,y) corresponds\n",
    "            to the feature id (x) and the local weight (y). The list is sorted\n",
    "            by decreasing absolute value of y.\n",
    "            score is the R^2 value of the returned explanation\n",
    "            local_pred is the prediction of the explanation model on the original instance\n",
    "        \"\"\"\n",
    "\n",
    "        weights = self.kernel_fn(distances)\n",
    "        labels_column = neighborhood_labels[:, label]\n",
    "        used_features = self.feature_selection(neighborhood_data,\n",
    "                                               labels_column,\n",
    "                                               weights,\n",
    "                                               num_features,\n",
    "                                               feature_selection)\n",
    "        if model_regressor is None:\n",
    "            model_regressor = Ridge(alpha=1, fit_intercept=True,\n",
    "                                    random_state=self.random_state)\n",
    "        easy_model = model_regressor\n",
    "        easy_model.fit(neighborhood_data[:, used_features],\n",
    "                       labels_column, sample_weight=weights)\n",
    "        prediction_score = easy_model.score(\n",
    "            neighborhood_data[:, used_features],\n",
    "            labels_column, sample_weight=weights)\n",
    "\n",
    "        local_pred = easy_model.predict(neighborhood_data[0, used_features].reshape(1, -1))\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Intercept', easy_model.intercept_)\n",
    "            print('Prediction_local', local_pred,)\n",
    "            print('Right:', neighborhood_labels[0, label])\n",
    "        return (easy_model.intercept_,\n",
    "                sorted(zip(used_features, easy_model.coef_),\n",
    "                       key=lambda x: np.abs(x[1]), reverse=True),\n",
    "                prediction_score, local_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
